---
title: 'OHI 2019 - Tourism and Recreation '
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 1
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../workflow/templates/ohi_hdr.html' 
  pdf_document:
    toc: true
editor_options: 
  chunk_output_type: console
---


[REFERENCE RMD FILE](https://cdn.rawgit.com/OHI-Science/ohiprep/master/globalprep/tr/v2018/tr_data_prep.html)


# Summary
This document describes the steps for obtaining the data used to calculate the tourism and recreation goal for the 2019 global assessment.

The general calculation is:
tr = Ep * Sr * Tw
and
Xtr = tr/90th quantile across regions

* Ep = Proportion of workforce directly employed in tourism
* Sr = (S-1)/5; Sustainability of tourism
* Tw = A penalty applied to regions with travel warnings from the US State Department (or Canada's Government Travel Advise and Advisory)


## The following data are used:

* Tourism sustainability: Travel and Tourism Competitiveness Index (TTCI) from World Economic Forum (WEF) (NOT updated for 2019)
* Proportion of workforce directly employed in tourism: World Travel & Tourism Council ([WTTC](https://www.wttc.org/datagateway))
* Travel warnings: ([U.S. State Department](https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html/) and [Canadian Government](https://travel.gc.ca/travelling/advisories))
* Per capita GDP: (World Bank with gaps filled using CIA data), used to gapfill missing values in Tourism sustainability (in previous years)


# Updates from previous assessment

## Tourism employment
WTTC data includes projections 10 years in the future, and in 2018 it was unclear when these projections began, so they used 2017 as their maximum data year. When downloading data for the 2019 assessment it was clear from the WTTC data gateway where the real data ended and projections began, so 2019 was used as the maximum data year.

We also updated the code to account for uninhabited/low population areas, as we did with the Artisanal Fishing Opportunities data prep script.

## Travel warnings
Previously, US state department data only were used to identify travel warnings for each country. In 2019 we incorporated advisories from the Canadian government to fill in data for countries the US did not establish warnings for (such as the US itself). The Canadian advisories were matched to the numeric scale used by the US State Department since 2018 (1-4, ranges from level 1 (normal precautions) to level 4 (do not travel)). 

Data on the US State Dept website span 2018 and 2019, but all of these will be considered advisory data for 2019, regardless of when they were created.

In 2019 we also gapfilled travel warnings for missing territorial regions, using administrative country data. In 2018 a multiplier of 1 was applied to each region with no data. In 2019 all of these regions also received a multiplier of 1 based on the admin country advisories.


**We were able to update the following data:**

* Proportion of jobs in tourism - WTTC data reported until 2029, but 2019 is most recent year of real data (year_max) (downloaded from WTTC on 07/08/2019)
* Travel warnings for 2019 (downloaded from U.S State Department and Canadian Government on 07/02/2019)

Tourism sustainability data from the WEC Travel and Tourism Competitiveness Report were not updated, as the 2019 report has not been released as of 15 July 2019. 

## Initial set-up code

```{r setup, message=FALSE, warning=FALSE, results="hide"}

#library(devtools)
#devtools::install_github("ohi-science/ohicore@dev")
library(ohicore)
library(tidyverse)
library(stringr)
library(WDI)
library(here)
library(janitor)
library(plotly)

source('https://raw.githubusercontent.com/OHI-Science/ohiprep_v2019/gh-pages/workflow/R/common.R')

## maximum year of wttc data:
year_max <- 2019

source(here("globalprep/tr/v2019/R/tr_fxns.R"))


```


# Ep: Proportion of workforce directly employed in tourism

These data are from the [World Travel & Tourism Council](http://www.wttc.org/datagateway).  We use "direct" employment data (see mazu: git-annex/globalprep/_raw_data/WTTC/d2019/README.md for instructions on obtaining data). The data extend to 2029, which includes 10 years of projections. The actual data goes to 2019 (projected/real data are differentiated on the data gateway chart).

These data are cleaned and formatted using the R/process_WTTC.R script. Missing values are gapfilled using the UN georegion information.

```{r wttc prop tourism, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}

## describe where the raw data are located:
scenario_yr <- "v2019"
dir_wttc <- file.path(dir_M, 'git-annex/globalprep/_raw_data/WTTC/d2019/raw')
dir_github <- here("globalprep/tr", scenario_yr)

## processing script that formats the WTTC for OHI, saves the following: intermediate/wttc_empd_rgn.csv
source(here("globalprep/tr/v2019/R/process_WTTC.R"))
##### v2019: Duplicates = Guadeloupe/Martinique, PR/VI, China (accounted for in the script)

## read in the dataset created by above function:
tr_jobs_pct_tour <- read_csv(here('globalprep/tr/v2019/intermediate/wttc_empd_rgn.csv')) %>% 
 dplyr::select(rgn_id, year, jobs_pct)

## format data to have complete years/regions and convert percentage of jobs to proportion of jobs
rgn_names <- read_csv('https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/eez/spatial/regions_list.csv') %>%
    dplyr::select(rgn_id)

# create data frame with a row for each combination of region id and year
rgn_names <- expand.grid(rgn_id = rgn_names$rgn_id, 
                             year= min(tr_jobs_pct_tour$year):max(tr_jobs_pct_tour$year)) 
      
tr_data_raw <- rgn_names %>%
  full_join(tr_jobs_pct_tour %>%
                rename(Ep = jobs_pct) %>%
                mutate(Ep = Ep/100) %>%
                mutate(Ep = ifelse(Ep > 1, NA, Ep)),
              by = c('rgn_id', 'year')) %>%
  filter(!rgn_id == 213) %>% 
  filter(!rgn_id == 255) # ditch disputed regions and Antarctica

## v2019: >2500 NAs in Ep column
# summary(tr_data_raw)


## gapfill missing data using UN georegion data:
georegions       <- georegions
georegion_labels <- georegion_labels

tr_data_raw <- tr_data_raw %>%
  left_join(georegions, by = 'rgn_id') %>%
  left_join(georegion_labels, by = 'rgn_id') %>%
  select(-r0)

# Calculate two different gapfill columns using r2 and r1
tr_data_raw_gf <- tr_data_raw %>%
  group_by(year, r2) %>%
  mutate(Ep_pred_r2 = mean(Ep, na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(year, r1) %>%
  mutate(Ep_pred_r1 = mean(Ep, na.rm=TRUE)) %>%
  ungroup()

# first gapfill with r2, if no value available use r1; create column indicating whether value was gapfilled and if so, by what method.
tr_data_raw_gf <- tr_data_raw_gf %>%
  mutate(Ep_all = ifelse(is.na(Ep), Ep_pred_r2, Ep)) %>%
  mutate(Ep_all = ifelse(is.na(Ep_all), Ep_pred_r1, Ep_all)) %>% 
  mutate(gapfilled = ifelse(is.na(Ep) & !is.na(Ep_all), "gapfilled", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & !is.na(Ep_pred_r2), "UN georegion (r2)", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & is.na(Ep_pred_r2) & !is.na(Ep_pred_r1), "UN georegion (r1)", method)) 

######################################

### After gap-filling, make sure low/uninhabited regions are NA
# Create df for unpopulated/low populated regions
low_pop()
low_pop <- low_pop %>%
  filter(est_population < 3000 | is.na(est_population)) %>%  #filter out regions that have populations > 3000 and keep NA values 
  rename(rgn_label = rgn_nam)


# make sure all the NAs are uninhabited regions
tr_data_nas <- tr_data_raw_gf %>% 
  filter(is.na(Ep_all)) %>% 
  select(rgn_id, year, r1_label, r2_label, rgn_label) %>% 
  left_join(low_pop, by = c("rgn_id", "rgn_label"))
#  filter(!duplicated(rgn_id))

max(tr_data_nas$est_population, na.rm=TRUE)<3000 # should be true

# make sure all the uninhabited regions are NA (along with gapfill and method if they were gapfilled above)
tr_data_raw_gf <- tr_data_raw_gf %>% 
  mutate(Ep_all = ifelse(rgn_id %in% low_pop$rgn_id, NA, Ep_all)) %>% 
  mutate(gapfilled = ifelse(is.na(Ep_all), NA, gapfilled)) %>% 
  mutate(method = ifelse(is.na(Ep_all), NA, method))


# check NAs once more 
# summary(tr_data_raw_gf)
# v2019: Adding the low pop df identifies 13 additional regions that should be NA instead of gapfilled, taking the total number of NAs in the data set from 245 to 700


# save the gapfill report data
tr_data_gf <- tr_data_raw_gf %>%
  select(rgn_id, year, gapfilled, method) 

write_csv(tr_data_gf, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism_gf.csv"))

tr_data <- tr_data_raw_gf %>%
  select(rgn_id, year, Ep=Ep_all) 



# Save gap-filled data
write_csv(tr_data, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv"))

```

## Data check and outlier investigation
```{r data check, message=FALSE, echo=FALSE}


## A quick check to make sure last year's values aren't too crazy different
## (NOTE: the source data has been updated, so there are some changes, but they shouldn't be super different)

old <- read_csv(here('globalprep/tr/v2018/output/tr_jobs_pct_tourism.csv')) %>%
  select(rgn_id, year, ep_old=Ep)

new <- read_csv(here('globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv')) %>%
  left_join(old) %>%
  filter(year==2018) %>%
  arrange(ep_old)

# Visualize data comparison 
ggplotly(ggplot(new, aes(x = Ep, y = ep_old, labels = rgn_id)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red"))


######## investigate outliers (if applicable)

new_diff <- new %>%
  mutate(difference = ep_old-Ep) %>% 
  filter(!is.na(ep_old)) %>% 
  filter(!is.na(Ep))

outliers <- tr_data_raw %>% 
  filter(rgn_id %in% c(15,116,117,118,119,122,125,127,173,250)) %>% 
  filter(year == 2018) %>% 
  select(rgn_id, rgn_label) %>% 
  left_join(new, by = "rgn_id", "year") %>% 
  mutate(difference = Ep-ep_old) %>% 
  mutate(avg_diff = mean(new_diff$difference))

### v2019: These outliers can be explained by changes in the source data. Compared raw data from v2018 and all of these regions have large changes in the % share of total employment reported for the year 2018. WTTC likely backfills and adjusts their data on a yearly basis.  
```



# Tw: Travel warnings

Primary source of information is from the [U.S. State Department](https://travel.state.gov/content/passports/en/alertswarnings.html), secondary source is the [Canadian Government](https://travel.gc.ca/travelling/advisories)


**For future assessments** It would be worthwhile to see if data can be "scraped" directly from the government websites into R. This seems possible given the new format of the state department travel warning data.

### Getting data for 2019 assessment
The following code is used transform the warnings into a multiplier that is used to calculate tourism and recreation scores. Data from each country are copied from the US and Canada government travel websites, pasted into an excel file, and saved as a .csv in the raw folder (tr_travelwarning_20??_raw.csv)

*Date downloaded*: 1 July 2019

*Date range of warnings*: 18 June 2018 - 1 July 2019 (note: regardless of date of the warning, the advisory year will be the assessment year)


## After raw data are uploaded, wrangle and clean the new data:

```{r, eval=FALSE}
##Reading and wrangling 2019 warning data

warn_raw <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2019_raw.csv'), na.strings = " ") %>% 
  mutate(country = as.character(country)) 

# Remove text information from level and filter out regional warnings
warn_clean <- warn_raw %>% 
  mutate(level = as.numeric(str_extract(level, '[1,2,3,4]'))) %>% 
  filter(!(regional %in% 1)) %>% # remove regions that have regional warnings, as those are no longer considered in the assessment
  select(assess_year, level, country) %>% 
  rename(year = assess_year)


## Correct regions that are reported together - check to make sure these are necessary and that everything is covered as data change from year to year. Also make sure level data is coming through for each. 


french_indies <- data.frame(country="French West Indies", 
                            country_new =c("Northern Saint-Martin")) %>%
  left_join(filter(warn_clean, country=="French West Indies")) %>%
  select(country=country_new, year, level)

BES <- data.frame(country="Bonaire, Sint Eustatius and Saba", 
                            country_new =c("Saba", "Sint Eustatius")) %>% # Bonaire already reported separately 
  left_join(filter(warn_clean, country=="Bonaire, Sint Eustatius and Saba")) %>%
    select(country=country_new, year, level)


line <- data.frame(country="Line Islands (Kiribati)", 
                            country_new =c("Line Group", "Phoenix Group")) %>%
  left_join(filter(warn_clean, country=="Line Islands (Kiribati)")) %>%
    select(country=country_new, year, level)
# These are not the region names reported in OHI, but they are used in the name_2_rgn function
  
warn_improved <- filter(warn_clean, country != "French West Indies") %>%
  bind_rows(french_indies) 

warn_improved <- filter(warn_improved, country != "Bonaire, Sint Eustatius and Saba") %>%
  bind_rows(BES)

warn_improved <- filter(warn_improved, country != "Line Islands (Kiribati)") %>%
  bind_rows(line)



##Correct names for regions not identified by the name_2_region, based off of error messages after running function below
# Change names to match those reported by OHI (not always necessary)
warn_improved <- warn_improved %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Israel"), "Israel", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country, "^Republic of the Congo"), "Republique du Congo", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"U.S. Virgin Islands"), "Puerto Rico and Virgin Islands of the United States", country)) %>% # creates duplicate of PR/VI with same warning level
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Puerto Rico"), "Puerto Rico and Virgin Islands of the United States", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Guadeloupe"), "Guadeloupe and Martinique", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Saint Vincent and The Grenadines"), "Saint Vincent and the Grenadines", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Burma"), "Myanmar", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"North Korea"), "North Korea", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Solomon Island"), "Solomon Islands", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Guam"), "Northern Mariana Islands and Guam", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Northern Mariana Islands"), "Northern Mariana Islands and Guam", country))
  

# Look at warn_improved and remove other regions that are duplicated or aren't reported in the OHI. Make sure there are no NAs. 
get_dupes(warn_improved)

warn_improved <- warn_improved %>% 
  filter(!duplicated(country))

write_csv(warn_improved, here("globalprep/tr/v2019/intermediate/warning_2019.csv"))


```


## Transform the warnings into a multiplier that is used to calculate tourism and recreation scores.

Travel warning  | Multiplier   | Description
--------------- | ------------ | -------------------
Level 1 | 1 (no penalty) | Exercise Normal Precautions: This is the lowest advisory level for safety and security risk. There is some risk in any international travel. 
Level 2 | 1 (no penalty) | Exercise Increased Caution:  Be aware of heightened risks to safety and security. 
Level 3 | 0.25  | Reconsider Travel: Avoid travel due to serious risks to safety and security.
Level 4 | 0 (full penalty, results in zero scores) | Do Not Travel:  This is the highest advisory level due to greater likelihood of life-threatening risks. 

```{r, eval=FALSE}
warn_complete <- read_csv(here("globalprep/tr/v2019/intermediate/warning_2019.csv"))

scores <-  data.frame(level = c(1, 2, 3, 4), multiplier = c(1, 1, 0.25, 0)) 


warn_multiplier <-  warn_complete %>%  
  left_join(scores, by="level") %>% 
  group_by(year, country) %>%
  mutate(warning_count = n()) %>%
  ungroup()

# Check to see if there are regions with more than one warning (in general there should be no regions with more than one advisory, but some are combined after being reported separately and may have different advisories) 

warn_count <- filter(warn_multiplier, warning_count>1)

# If warn_count >0, multipliers from duplicate regions will be averaged: 
warn_multiplier <- warn_multiplier %>%
  group_by(year, country) %>%
  summarize(multiplier = mean(multiplier))

#Save file with 2019 multiplier data
write_csv(warn_multiplier, here("globalprep/tr/v2019/intermediate/warning.csv"))

```


## Convert names to OHI regions and clean. 

```{r travel warnings, message=FALSE, echo=FALSE, results="hide"}
warn <- read_csv(here("globalprep/tr/v2019/intermediate/warning.csv"))

#Add rgn_id
warn_rgn <- name_2_rgn(df_in = warn, 
                       fld_name='country', 
                       flds_unique=c('country','year'))
# Double check error message to make sure all of the landlocked regions are indeed landlocked, and that everything not found in the lookup table is not a region reported by OHI. 
                                                  
# Check to see if any regions are duplicated:
sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))
# China has multiple warnings (rgn_id 209)

# Average warnings for China, China Macao SAR and Hong Kong: 
warn_rgn <- warn_rgn %>%
  group_by(rgn_id, rgn_name, year) %>%
  summarize(multiplier = mean(multiplier)) %>%
  ungroup()

# Check again to see if there are any duplicate multipliers
sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))

```


## Final step: Compare with previous year's data

Many European regions now have a travel warning due to increased terrorism (e.g., United Kingdom, Italy, Spain, Germany), although this doesn't show up in the following figure because previously, these regions had no travel warning (and were thus, NA).

The change in not penalizing subregional warnings tended to reduce the penalty (i.e. increase the multiplier value).
```{r, message=FALSE, echo=FALSE}
# The following indicates changes over time as well as changes to the State Department's approach to quantifying risk

tmp_old <- read_csv(here("globalprep/tr/v2018/output/tr_travelwarnings.csv")) %>% 
  spread(year, multiplier) %>%
  data.frame()
  
tmp <- warn_rgn %>%
  spread(year, multiplier) %>%
  data.frame() %>% 
  left_join(tmp_old, by = "rgn_id")

# Add noise to scatter to compare warnings between this year and last year (clustered points indicate where there are multiple observations of the same value)
ggplotly(ggplot(tmp, aes(x = jitter(X2018), y = jitter(X2019), labels = rgn_id)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red"))

# Investigate outliers (will potentially change from year to year)
outliers <- tmp %>% 
  filter(rgn_id %in% c(49, 73, 112, 136, 139)) %>% 
  select(rgn_id, rgn_name, X2018, X2019)

# Makes sense; these regions have all experienced stability changes in the past year 

```

## Gapfill territorial regions with admin country data and save the travel warning data in the output folder

```{r, eval=FALSE}
georegions <- georegion_labels %>%
  select(rgn_id)
  
warn_rgn_spread <- warn_rgn %>%
  spread(year, multiplier) %>%
  full_join(georegions, by=c("rgn_id")) %>%
  data.frame() %>%
  gather(year, multiplier, starts_with("X")) %>%
  mutate(year = gsub("X", "", year)) %>%
  filter(rgn_id <= 250) %>%
  filter(rgn_id != 213) # Filter out Antarctica
  
# Check number of regions reported - should be 220 
table(warn_rgn_spread$year) 


# Identify territories without advisories and connect them with multipliers for admin regions 
region_data() # reload common.R if this isn't working
warn_rgn_nas <- warn_rgn_spread %>% 
  filter(is.na(rgn_name)) %>% 
  select(rgn_id) %>% 
  left_join(rgns_eez, by = "rgn_id") %>% 
  select(rgn_id, rgn_name, admin_rgn_id, admin_country_name)

admin_rgn_multipliers <- warn_rgn_nas %>% 
  select(rgn_id = admin_rgn_id) %>% 
  left_join(warn_rgn_spread, by = "rgn_id") %>% 
  rename(admin_rgn_id = rgn_id) %>% 
  select(admin_rgn_id, year, multiplier) %>%
  filter(!duplicated(admin_rgn_id))

warn_rgn_nas <- warn_rgn_nas %>% 
  left_join(admin_rgn_multipliers, by = "admin_rgn_id") %>% 
  select(rgn_id, rgn_name, year, multiplier)


### Finalize warnings data and save
# Remove NAs from warn_rgn_spread, then add them back in

warn_rgn_spread <- warn_rgn_spread %>%
  filter(!is.na(rgn_name)) %>% 
  bind_rows(warn_rgn_nas)
  
warn_rgn_all_rgns <- warn_rgn_spread %>%
  select(rgn_id, year, multiplier) %>%
  arrange(year, rgn_id) %>% 
  mutate(year = as.numeric(year))

# Check again that we have 220 regions reported
table(warn_rgn_all_rgns$year) 


# Save 2019 data
write_csv(warn_rgn_all_rgns, here('globalprep/tr/v2019/output/tr_travelwarnings_2019only.csv'))


## Create gapfill file
# Add information about gapfill method to regions that were filled based on administrative country advisory above

travelwarning_gf <- read_csv(here("globalprep/tr/v2019/output/tr_travelwarnings.csv")) %>% 
  mutate(gapfilled = ifelse(rgn_id %in% warn_rgn_nas$rgn_id, 1, 0)) %>% 
  mutate(method = ifelse(gapfilled == 1, "Gapfilled based on administrative country advisory", NA)) %>% 
  select(-multiplier)

write_csv(travelwarning_gf, here('globalprep/tr/v2019/output/tr_travelwarnings_gf.csv'))


## Combine with previous years' data and save

travel_warnings_all <- read_csv(here("globalprep/tr/v2018/output/tr_travelwarnings.csv")) %>%
  bind_rows(warn_rgn_all_rgns)

write_csv(travel_warnings_all, here('globalprep/tr/v2019/output/tr_travelwarnings.csv'))

```

