---
title: 'OHI `r format(Sys.Date(), "%Y")`: Sea Surface Temperature Pressure Layer'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../workflow/templates/ohi_hdr.html'
  pdf_document:
    toc: true
---

# Summary

This script .rmd creates the Sea Surface Temperature (SST) anomaly threshold rasters for the prs_sst data layer. This notebook was created by Carlo Broderick based on the previous notebook code and structure in the beautiful summer of 2023. This notebook and other changes to the structure and process of the prs_sst dataprep are the result of a structural shift in how this layer is calculated. To read more on the background of this change refer to the github issue: <https://github.com/OHI-Science/globalfellows-issues/issues/244>

This .rmd should not be run every year, the sea surface temperature anomaly threshold rasters should only be created if the [CoRTAD version 6](https://www.ncei.noaa.gov/products/coral-reef-temperature-anomaly-database) dataset has been updated to version 7. This is because the historical data for this data set does not change and thus the standard deviation and mean SST for the reference period from 1982 - 2011 do not change. Only if historical SST numbers are updated should these rasters be recalculated.

In addition to this .rmd, there is an R script in the background_job_scripts folder that has the same code present in this .rmd. This script will allow you to run this computation as a background job which frees your console for other more important, fulfilling activities.

------------------------------------------------------------------------

# Data Source

Data come from NOAA's NCEI: [CoRTAD version 6](https://www.ncei.noaa.gov/products/coral-reef-temperature-anomaly-database)

See `prs_sst/v2015/dataprep.R` for preparation of the "annual_pos_anomalies" data.

**Native Data Resolution**: \~4km\
**Description**: Cortadv6_SSTA.nc = SST anomalies (weekly SST minus weekly climatological SST), weekly data for all years, degrees Kelvin Cortadv6_weeklySST.nc = SST, weekly data for all years, degrees Kelvin\
**Time Range**: 1982 - 2021 (weekly averages across all years)\
**Format**: NetCDF **Downloaded**: June 22, 2022

------------------------------------------------------------------------

# Methods

1.  Extreme events per year based calculated as number of times SST anomaly exceeds one SST Standard Deviation above mean SST based on weekly values (annual_pos_anomalies data, see v2015/dataprep.R for analysis).
2.  Sum extreme events for five year periods to control for yearly variation.
3.  Change in extreme events: Subtract number of extreme events for each five year period from control period (1985-1989). 85-89 was used instead of 82-86 due to non-normative weather data during those years.
4.  Rescale "Change in extreme events" data to values between 0 and 1 by dividing by the 99.99th quantile among all years of data.

## Setup

```{r setup, message=F,warning=F, eval = FALSE}

# Set options for code chunks
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'figs/', message = FALSE, warning = FALSE)

# Load packages
library(raster)
library(RColorBrewer)
library(tidyverse)
library(rgdal)
library(doParallel)
library(foreach)
library(sf)
library(ncdf4)
library(httr)
library(lubridate)
library(animation)
library(plotly)
library(here)
library(snow)
library(terra)

# OHI spatial files, directories, etc
source(here("workflow/R/common.R"))

# Update scenario year, set up programatic scenario year updating
scen_year_number <- 2023
scen_year <- as.character(scen_year_number)
prev_scen_year <- as.character(scen_year_number - 1)

# Standard OHI file paths
dir_data <- paste0(dir_M, "/git-annex/globalprep/_raw_data/CoRTAD_sst/d", scen_year)
dir_int  <- paste0(dir_M, "/git-annex/globalprep/prs_sst/v", scen_year, "/int")

# Change this file path for full update to correct update scenario year
dir_rasters <- paste0(dir_M, 
                      "/git-annex/globalprep/prs_sst/prs_sst_calculated_rasters/v2023_update")

# Load in OHI spatial data
ohi_rasters()
regions_shape()

# Specify years for SD and mean calculation
yrs <- 1982:2011

# Rainbow color scheme and load in OHI regions
cols <- rev(colorRampPalette(brewer.pal(11, 'Spectral'))(255)) 
land <- regions %>% subset(rgn_type %in% c("land", "land-disputed", "land-noeez"))

```

------------------------------------------------------------------------

## Get new data if available

```{r get new data, eval = FALSE}

# Define the base URL for the data
url <- "https://data.nodc.noaa.gov/cortad/Version6"

# Define the URL for the SSTA and 
weekly_sst <- sprintf("%s/cortadv6_WeeklySST.nc", url)

# Define the local file path for SSTA and WeeklySST data
weekly_sst_filename <- paste0(dir_M, "/git-annex/globalprep/_raw_data/CoRTAD_sst/d", scen_year, "/cortadv6_WeeklySST.nc")

# Download the SSTA WeeklySSTA and save it to the file path, (~100GB+, ~30GB+)
weekly_sst_res <- httr::GET(weekly_sst, write_disk(weekly_sst_filename))

# Close all open connections
closeAllConnections()

```

------------------------------------------------------------------------

## Generate annual positive anomalies

We consider anomalies the mean plus one standard deviation; these are the thresholds used to identify 'extreme events'. Since the sea surface temperature anomaly data downloaded from CoRTAD is just the mean, we calculate standard deviation and count cases where the anomaly data exceeds the standard deviation (?)

```{r read in and fomat data, eval = FALSE}

# Load netcdf for SST data
weekly_sst <- rast(list.files(dir_data, 
                              pattern = "WeeklySST.nc", 
                              full.names = TRUE), 
                   subds = "WeeklySST")

# Rename each layer, terra autonames so we take the names from layer dates
names(weekly_sst) <- time(weekly_sst)

# Assign names to an object
names_weekly <- names(weekly_sst)

# Convert the names into a data frame
sst_df <- data.frame(name = names_weekly) %>%
    # Convert the name to a date
    mutate(date = as.Date(name)) %>%
    # Extract the year, month, day, and week
    mutate(
        year = year(date),
        month = month(date),
        day = day(date),
        week = week(date)
    )

```

## \~15 min per layer, 53 layers, 5 cores (2023)

This loop takes the weekly SST data and creates 53 standard deviation raster files, one for each week of the year, and saves them to Mazu. Each weekly raster file's cell values correspond to the standard deviation SST values for all years in the reference period (1982 - 2011) for that week. This gives us a measure of variability in SST for that time of the year that will allow us to identify SST values that are "anomalous" for that time of the year.

```{r Weekly SD raster loop, eval = FALSE}

# Loop through 53 weeks
for(i in 1:53){
  
  # Start timer with tic()
  start_time <- Sys.time()
  
  # Print time message
  print(paste("anom_threshold for week", i, "started at", start_time))
  
  # Initialize r
  r <- NULL
  
  # Loop through all the years in the yrs variable
  for (j in yrs){
    
    # Find the index of the weekly data for the year j and week i
    w <- which(substr(names_weekly, 1, 4) == j)[i]
    
    # If there is no data for this year and week, skip to the next iteration of the loop
    if(is.na(w)) next
    
    # Fill r with weekly rasters
    if (is.null(r)) {
      r <- weekly_sst[[w]]
    } else {
      r <- c(r, weekly_sst[[w]])
    }
    
  } # End of inner for-loop
  
  # Create standard deviation raster from week raster stack
  terra::app(r,
             fun = "sd",
             na.rm = TRUE,
             filename = file.path(dir_rasters, "weekly_sd_30yr_rasters", sprintf("sst_sd_week_%s.tif", i)),
             overwrite=TRUE,
             cores = 7) # Adjust the cores
  
  # End timer and calculate elapsed time
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  
  # Print time message
  print(paste("anom_threshold for week", i, "started at", start_time, "and took", elapsed_time, "minutes to complete"))
  
  # Garbage collection to free up memory
  gc()
  
} # End of outer for-loop

```
