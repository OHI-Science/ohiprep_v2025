---
title: 'OHI 2024: Ocean Acidification Data Prep'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
format:
  html:
    code-fold: show
    toc: true
    toc-depth: 3
    toc-float: true
    number-sections: false
    theme: cerulean
    highlight-style: haddock
    include-in-header: '../../../src/templates/ohi_hdr.html'
  pdf:
    toc: true
editor: 
  markdown:
    wrap: sentence
  chunk_output_type: inline
---

# Summary

**Overview**

The [data source product](https://data.marine.copernicus.eu/product/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/description) corresponds to a REP L4 time series of monthly global reconstructed surface ocean pCO2, air-sea fluxes of CO2, pH, total alkalinity, dissolved inorganic carbon, saturation state with respect to calcite and aragonite, and associated uncertainties on a 0.25° x 0.25° regular grid.
The product is obtained from an ensemble-based forward feed neural network approach mapping situ data for surface ocean fugacity (SOCAT data base, Bakker et al. 2016, <https://www.socat.info/>) and sea surface salinity, temperature, sea surface height, chlorophyll a, mixed layer depth and atmospheric CO2 mole fraction.
Sea-air flux fields are computed from the air-sea gradient of pCO2 and the dependence on wind speed of Wanninkhof (2014).
Surface ocean pH on total scale, dissolved inorganic carbon, and saturation states are then computed from surface ocean pCO2 and reconstructed surface ocean alkalinity using the CO2sys speciation software \[See: Citation information\].

Specifically, we want to evaluate aragonite saturation.
This is because it is a good metric for ocean acidification pressure on marine organisms.
Some sources referring to this include:

[Ocean Acidification \| Learn Science at Scitable](https://www.nature.com/scitable/knowledge/library/ocean-acidification-25822734/)

1)  $CO_2(aq) + H_2O ↔ H_2CO_3$
2)  $H_2CO_3 ↔ HCO_3^- + H^+$
3)  $HCO_3^- ↔ CO_3^{2-} + H^+$
4)  $CO_2(aq) + CO_3^{2-} + H_2O → 2HCO_3^-$

For **supersaturated** (Ω \> 1) conditions, with an excess of \[ $CO_3^{2-}$ \]seawater, a crystal of $CaCO_3$ will tend to grow, and in **undersaturated** (Ω \< 1) conditions, $CaCO_3$ will dissolve.
\[$CO_3^{2-}$\]saturation is a weak function of temperature and salinity but a strong function of water depth (pressure) meaning that deeper waters are typically more corrosive.
How \[saturation level\] affects calcifying organisms is difficult to predict however, because different species exert varying degrees of biological control on the calcification process.
**Therefore, it may be a better idea to look at the current level of aragonite.**

[Ocean Acidification: Saturation State - Science On a Sphere](https://sos.noaa.gov/catalog/datasets/ocean-acidification-saturation-state/#:~:text=Aragonite%20saturation%20state%20is%20commonly,organisms%20with%20calcium%20carbonate%20structures)

Aragonite saturation state is commonly used to track ocean acidification because **it is a measure of carbonate ion concentration**.
Aragonite is one of the more soluble forms of calcium carbonate and is [widely used by marine calcifiers]{.underline} (organisms with calcium carbonate structures).

[Climate Change Indicators: Ocean Acidity \| US EPA.](https://www.epa.gov/climate-indicators/climate-change-indicators-ocean-acidity#:~:text=The%20global%20map%20in%20Figure,data%20(2006%E2%80%932015).

Aragonite is a specific form of calcium carbonate that many organisms produce and use to build their skeletons and shells, and the saturation state is a measure of how easily aragonite can dissolve in the water.

The [**lower**]{.underline} the saturation level, the [**more difficult**]{.underline} it is for organisms to build and maintain their protective skeletons and shells.

Aragonite saturation has only been measured at selected locations during the last few decades, but it can be calculated reliably for different times and locations based on the relationships scientists have observed among aragonite saturation, pH, dissolved carbon, water temperature, concentrations of carbon dioxide in the atmosphere, and other factors that can be measured.

### Updates from 2024 assessment

Copernicus Marine Service's Global Ocean Surface Carbon product was found by Melanie Frazier, which has been an excellent resource.
This layer is a revamped version of previous years', and can be updated in future years now. In the future, I would finish updating the script by changing certain `raster::calc` and `raster::stack` functions (etc) to the `terra` package.

# Data Source

### Classification

**Reference:** [Copernicus Marine Service](https://data.marine.copernicus.eu/product/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/description)

**Downloaded:**

**Description**: Aragonite Saturation State $\Omega_{arg}$

**Full name:** Global Ocean Surface Carbon

**Product ID:** MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008

**Source:** In-situ observations

**Spatial extent:** Global OceanLat -88.12° to 89.88°Lon -179.87° to 179.88°

**Spatial resolution:** 0.25° × 0.25°

**Temporal extent:** 31 Dec 1984 to 30 Nov 2022

**Temporal resolution:** Monthly

**Processing level:** Level 4

**Variables:** Dissolved inorganic carbon in sea water (DIC), Sea water pH reported on total scale (pH), Surface partial pressure of carbon dioxide in sea water (spCO2), Surface downward mass flux of carbon dioxide expressed as carbon (fpCO2), Total alkalinity in sea water

**Feature type:** Grid

**Blue markets:** Conservation & biodiversity, Climate & adaptation, Science & innovation, Marine food

**Projection:** WGS 84 / World Mercator (EPSG 3395)

**Data assimilation:** None

**Update frequency:** Annually

**Format:** NetCDF-4

**Originating centre:** LSCE (France)

**Last metadata update:** 30 November 2023

# Methods

1.  Set-up source and file paths

2.  Download data needed

-   Automated download of new data
-   Automated download of historical data

3.  Split the global OA MultiLayer NetCDF into its individual raster layers, which are by month.

-   This would be saved in Mazu, within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_monthly_rasters`

4.  Raster calculations for historical and new data

-   Create a raster of the average historical values by making a `terra` RasterBrick and calculate the average over the reference years (1985 - 2000)
    -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int`
-   Create annual mean rasters for the new data by stacking the monthly rasters by year and using `raster::calc` to calculate the mean for that year.
    -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_annual_mean`

5.  Rescale each annual raster between 0 to 1 using the historical average data as a reference -- v2024 updated the function

6.  Project, resample, and check the extent of the new data, historical ref data, and zones raster from OHI

7.  Calculate Zonal Statistics using the "mean" between the zones raster and the rescaled annual rasters for each region.
    Finish by saving the dataframe within `/home/lecuona/OHI_Intro/ohiprep_v2024/globalprep/prs_oa/v2024/output`.

# Setup

```{r setup, message = FALSE, warning = FALSE, verbose = FALSE}

#set options for all chunks in code
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval=FALSE)

# load packages
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}
librarian::shelf(
  here,
  CopernicusMarine,
  janitor,
  raster,
  terra,
  maps,
  httr,
  jsonlite,
  purrr,
  tictoc,
  sf, 
  googleVis,
  RColorBrewer,
  foreach,
  doParallel, # for using multiple cores, if needed
  tidyverse, 
  ohicore,
  ggplot2,
  reticulate, # for python coding
  ncmeta,
  ncdf4,
  plotly
)

# ---- sources! ----
source(here("workflow", "R", "common.R")) # file creates objects to process data

# ---- set year and file path info ----
current_year <- 2024 # Update this in the future!!
version_year <- paste0("v",current_year)
data_dir_version_year <- paste0("d", current_year)

# ---- data directories ----

# Raw data directory (on Mazu)
raw_data_dir <- here::here(dir_M, "git-annex", "globalprep", "_raw_data")

# CMS (Copernicus Marine Service) raw data directory
cms_dir <- here(raw_data_dir, "CMS", data_dir_version_year)

# prs_oa dir
oa_dir <- here(dir_M, "git-annex", "globalprep", "prs_oa", version_year)

# output data dir for intermediate data products
int_dir <- here(oa_dir, "int")
# dir.create(int_dir) # to create the path on Mazu if it has not already been done

# final output dir
output_dir <- here("globalprep","prs_oa", version_year, "output")

# set colors
cols = rev(colorRampPalette(brewer.pal(9, 'Spectral'))(255)) # rainbow color scheme

# Read in eez raster with cells at 1km -- used for resampling and zonal statistics (held on an NCEAS server)
zones <- terra::rast(here::here(dir_M, "git-annex", "globalprep","spatial","v2017","regions_eez_with_fao_ant.tif"))
```

## Install copernicusmarine tool in python

For the next two chunks, we will be using a Python Library API (Application Programming Interface) to automate the download of the data.

```{python}
# first, we need to install the CMS tool using python!

# these are "modules", much like packages in R
import sys
import subprocess

def install(package): # a function to download the copernicusmarine package
    subprocess.check_call([sys.executable, "-m", "pip", "install", package]) # `sys.executable` is the path to the python executable. for example, "/usr/bin/python3".  This makes the process more reproducible because on the server the sys.executable for python could be different based on the individual.
    #"-m" tells the Python interpreter to run a module, in this case "pip", as a script, using the new function "install" that we created using `def` and inputting the name of the function for the variable "package"

try:
    install("copernicusmarine") # use the function to download the package
    import copernicusmarine # will import the package copernicusmarine.
    # if the package is already installed, the rest of the code will be skipped
except ImportError:
    print(f"Installing copernicusmarine...") # if it does not work, this will print in the console. 

print(f"copernicusmarine version: {copernicusmarine.__version__}") # if correctly downloaded, then it will print the version
# v2024: copernicusmarine version: 1.3.1
```

# Automated download of raw data

## New Data Download (Copernicus Marine Service, 2010 - 2022; Downloaded August 1, 2024)

In Quarto documents, you are able to fluidly exchange between python and R code by specifying what language will be used in the chunk name.
This is extremely helpful for us, as it allows us to use the python API on the [CMS website](https://data.marine.copernicus.eu/product/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/download).
Here are the steps to download the data properly:

1.  Login to CMS: go to their website and make your username and password. It is quick and should be usable immediately after the credentials have been made.
2.  Go to <https://data.marine.copernicus.eu/product/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/description>. This is the link to the "Global Ocean Surface Carbon" product. Ensure that you read the overview and check for any changes from the previous year (references, characteristics, etc).
3.  Click the "Download data" button. FYI: the button itself does not have words on it, it is just an arrow pointing downwards. It is in the upper right hand corner of the page, next to the star and map icons.
4.  This will bring you to their graphical tool download page. For the [**Variables**]{.underline}, "Clear all" and select only "Aragonite saturation state in sea water: *omega_ar \[-\]*". Scroll to the bottom of the page, and under **Date Range**, specify 2010-01-01 at a time of 00:00:00 to your current year, 20xx-12-01 at a time of 00:00:00, to ensure you are obtaining the full year.
5.  After, instead of clicking the dark blue "Download" button, you will select the grey "Automate" button. Select Python API, and copy the `copernicusmarine.subset()` code. **This would be pasted starting from `copernicus.subset(...` to `...disable_progress_bar=False,`**

[![](python_api_code_ex.png){fig-align="center"}](https://data.marine.copernicus.eu/product/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/download)

```{python}
# This layer lags by 2 years, since we need a full year of data. 

# v2024: username <- slecuona; password <- OceanHealth2024

import os
import copernicusmarine

# set the credentials made on the CMS website
copernicusmarine.login(username='slecuona', password='OceanHealth2024') 
# v2024: INFO - 2024-07-30T16:04:43Z - Credentials file stored in /home/lecuona/.copernicusmarine/.copernicusmarine-credentials.

# define cms_dir, since the python chunk cannot read objects from previous R chunks/in your environment
dir_M = "/home/shares/ohi"
current_year = 2024 # UPDATE!
version_year = f"v{current_year}" # using an f-string
data_dir_version_year = f"d{current_year}"
raw_data_dir = os.path.join(dir_M, "git-annex", "globalprep", "_raw_data") # os.path.join is part of the os module and will merge into a single path
cms_dir = os.path.join(raw_data_dir, "CMS", data_dir_version_year) # for the specific CMS folder within _raw_data

# make sure the directory exists
os.makedirs(cms_dir, exist_ok=True) # will create the dir if it does not already exist

# use copernicusmarine to download the data --this is from the python API copyable code on the CMS site
# ============== Update each year!!!!================
copernicusmarine.subset( # start of copied code from CMS website
    dataset_id="dataset-carbon-rep-monthly",
    dataset_version="202311",
    variables=["omega_ar"],
    minimum_longitude=-179.875,
    maximum_longitude=179.875,
    minimum_latitude=-88.125,
    maximum_latitude=89.875,
    start_datetime="2010-01-01T00:00:00",
    end_datetime="2022-12-01T00:00:00",
    force_download=True,
    subset_method="strict",
    disable_progress_bar=False, # end of copied code from CMS website
    
    output_filename=os.path.join(cms_dir, "global_aragonite_saturation_raw.nc") ## This line is not copied!!! Need it to specify the Mazu directory for the data to download to
    
) # v2024 python API download code, which will download as a singular multi-layer NetCDF file (nc-file)

print(f"Data downloaded to: {os.path.join(cms_dir, 'global_aragonite_saturation_raw.nc')}") # print in console: data download finished!
```

## Historical Data Download

This will be used as a reference point when rescaling later on.
The mean of the historical data (1985 - 2000) from CMS will be used.
The download process is the same as for the new yearly data, however it has a different **Date Range** (1985-01-01T00:00:00 - 2000-12-01T00:00:00).
Each year, the only thing that should change is the "current_year" object, unless there has been a reevaluation and the dates used for the historical reference point is changed.

```{python}
import os
import copernicusmarine

# define cms_dir, since the python chunk cannot read objects from previous R chunks/in your environment
dir_M = "/home/shares/ohi"
current_year = 2024 # UPDATE!
version_year = f"v{current_year}" # using an f-string
data_dir_version_year = f"d{current_year}"
raw_data_dir = os.path.join(dir_M, "git-annex", "globalprep", "_raw_data") # os.path.join is part of the os module and will merge into a single path
cms_dir = os.path.join(raw_data_dir, "CMS", data_dir_version_year) # for the specific CMS folder within _raw_data

# make sure the directory exists
os.makedirs(cms_dir, exist_ok=True) # will create the dir if it does not already exist

# use copernicusmarine to download the data --this is from the python API copyable code on the CMS site
copernicusmarine.subset(
  dataset_id="dataset-carbon-rep-monthly",
  dataset_version="202311",
  variables=["omega_ar"],
  minimum_longitude=-179.875,
  maximum_longitude=179.875,
  minimum_latitude=-88.125,
  maximum_latitude=89.875,
  start_datetime="1985-01-01T00:00:00",
  end_datetime="2000-12-01T00:00:00",
  force_download=True,
  subset_method="strict",
  disable_progress_bar=True,
  
  output_filename=os.path.join(cms_dir, "historical_1985_2000_aragonite_saturation_raw.nc") ## This line is not copied!!! Need it to specify the Mazu directory for the data to download to
    
) # v2024 python API download code, which will download as a singular multi-layer NetCDF file for the historical data (nc-file)
  
print(f"Data downloaded to: {os.path.join(cms_dir, 'historical_1985_2000_aragonite_saturation_raw.nc')}") # print in console: data download finished!
```

## Manual download of raw data, if automation has issues -- not necessary!!

The rest of the code in the script does not use the manually downloaded data, but if it is necessary you can read it in using this method and proceed.
However, keep in mind that automation is more reproducible.

```{r}
# open the NetCDF file
# file_path <- here::here(raw_data_dir, "CMS", "d2024_manual", "dataset-carbon-rep-monthly_1722109120120.nc") # file path to the manually downloaded raw data
```

# Split into monthly rasters

```{r}
# ---- create directory for storing the monthly individual rasters ----
int_monthly_dir <- here::here(int_dir, "oa_monthly_rasters")
dir.create(int_monthly_dir, showWarnings = TRUE) # to create the directory in case it has not already been made in Mazu

# open the NetCDF file
raw_file_path <- here::here(cms_dir, "global_aragonite_saturation_raw.nc") # file path to the API downloaded raw data

# load the file as a SpatRaster to begin with, for greater usability with terra
oa_multi_raster <- terra::rast(raw_file_path)

# check the current CRS: will need to be reprojected to mollweide later, so that it can be clipped to the OHI eez layer
crs(oa_multi_raster) # v2024: ""GEOGCRS[\"WGS 84 (CRS84)\",\n".  
plot(oa_multi_raster) # to get a quick visualization of the multi-layer SpatRaster

# since nc_raster is already loaded as a SpatRaster object
# check the number of layers
num_layers <- terra::nlyr(oa_multi_raster)
print(num_layers) # v2024: 156 layers, which makes sense because the data goes from 2010 - 2022 monthly

# get the time attribute for each layer, to check that it goes from 2010 - 2022 (v2024)
time_attribute <- terra::time(oa_multi_raster)
print(time_attribute)

# Create a list to store individual rasters
individual_rasters <- list()

# loop through each layer and create individual rasters from the SpatRaster
for (i in 1:num_layers) {
  layer <- oa_multi_raster[[i]] # the layer in the seq from 1:156 (v2024)
  date <- time_attribute[i] # the month, in this case, of the raster
  layer_name <- format(date, "%Y-%m-%d") # the name for the .tif when it is saved
  individual_rasters[[layer_name]] <- layer # put them in the list 
}

# print the names of the individual rasters in the list, to ensure the loop worked
names(individual_rasters) # v2024: "2010-01-01" - "2022-12-01"

# save individual rasters to files on Mazu at `int_monthly_dir`
tic()
for (name in names(individual_rasters)) {
  output_file <- file.path(int_monthly_dir, paste0(name, ".tif")) # move each raster to the specified path in Mazu
  writeRaster(individual_rasters[[name]], filename = output_file, overwrite = TRUE) # if files already exist, overwrite them
  print(paste("Saved:", output_file)) # allows us to see the names of the files as they are being saved
} #great! check Mazu that files are all there
toc() # v2024: 20.528 sec elapsed

# let's take a look at one of the monthly rasters we just saved
rast_2010_02_01 <- rast(here::here(int_monthly_dir, "2010-02-01.tif")) # read it in and rasterize it using terra
plot(rast_2010_02_01, main = "Aragonite Saturation Raster - February 1, 2010", xlab = "Longitude", ylab = "Latitude") # plot it!

is.lonlat(rast_2010_02_01) # TRUE, so it does currently have a long/lat CRS
crs(rast_2010_02_01) # v2024: GEOGCRS[\"WGS 84\",\n. Once again, will have to reproject later.  
```

# Raster Calculations

## Historical Reference (take average of 1985 - 2000 values)

The historical mean of Aragonite Saturation from 1985 - 2000 will be used for comparison, to calculate the pressure scores.

```{r hist_mean}
# read in NetCDF file we downloaded using the API
oa_hist_multilayer_nc <- terra::rast(here::here(cms_dir, "historical_1985_2000_aragonite_saturation_raw.nc")) 

# check that the CRS is the same as the current data for rescaling later
crs(oa_hist_multilayer_nc) # v2024: yes, it is! "GEOGCRS[\"WGS 84 (CRS84)\",\n". We can move forward
ext(oa_hist_multilayer_nc) == ext(rast_2010_02_01) # TRUE, good!
res(oa_hist_multilayer_nc) == res(rast_2010_02_01) # TRUE TRUE, great! 
```

Calculate the local average (not global, see [documentation](https://www.rdocumentation.org/packages/terra/versions/0.8-6/topics/local)).

```{r}
# since it already multilayer and we plan on averaging across all years, we can make it a RasterBrick. It is similar to a RasterStack (that can be created with `stack()`), but processing time should be shorter when using a RasterBrick (from documentation)
oa_hist_moll_brick <- brick(oa_hist_multilayer_nc)

# Calculate the mean across all layers and years
tic()
oa_historical_average <- terra::mean(oa_hist_moll_brick) 
toc() # v2024: 169.212 sec elapsed

# write the result to a new raster file for future use
terra::writeRaster(oa_historical_average, 
            filename = here::here(int_dir, "oa_1985_2000_historical_average.tif"), # save to Mazu
            format = "GTiff",
            overwrite = TRUE)
```

## Annual mean aragonite saturation

Annual mean aragonite saturation rasters calculated from the monthly data.

```{r msla_monthly_to_annual, eval = F}
# ---- create directory for storing the monthly individual rasters ----
oa_annual_mean_dir <- here::here(int_dir, "oa_annual_mean")
dir.create(oa_annual_mean_dir, showWarnings = TRUE) # to create the directory in case it has not already been made in Mazu

# make a list of the already saved monthly rasters
oa_monthly_files <- list.files(path = here(int_monthly_dir),
                               pattern = "\\.tif$", # to ensure we only grab the .tif files, not the .tif.aux.json files
                               full.names = TRUE) # returns the full path, not just the file name

# the minimum year of the individual rasters
minyr <- substr(oa_monthly_files, 75, 78) %>% # taking the string that defines the year the raster's data was from 
  as.numeric() %>% 
  min()

# the maximum year of the individual rasters
maxyr <- substr(oa_monthly_files, 75, 78) %>% # taking the string that defines the year the raster's data was from 
  as.numeric() %>% 
  max() # take the max of all the years for the data downloaded

# check that it makes sense with the data you downloaded
paste0(minyr, "-", maxyr)

tic()
## stack all rasters for each year, calculate the annual mean, then write as a raster
registerDoParallel(6)
foreach(yr = c(minyr:maxyr)) %dopar% { # check that the minyr and maxyr are correct with what you downloaded
  
  files <- oa_monthly_files[str_detect(oa_monthly_files, as.character(yr))] # of the list earlier, detect the year from each file so that we can stack them by year
  
  rast_annual_mean <- stack(files) %>%
    calc(mean, na.rm = TRUE) %>% # calculate the average for each year for each cell in the raster over all the months, creating a raster of the average values for that year
    terra::writeRaster(filename = sprintf("%s/oa_annual_mean/oa_annual_%s.tif", int_dir, yr),
                       overwrite = TRUE)
}
toc() # v2024 - 8.114 sec elapsed

# check the crs, ext, and res of the files to ensure nothing changed
annual_2010 <- rast(here::here(oa_annual_mean_dir, "oa_annual_2010.tif")) # read it in and rasterize it using terra
plot(annual_2010, main = "Aragonite Saturation Annual Mean - 2010", xlab = "Longitude", ylab = "Latitude") # plot it!

is.lonlat(annual_2010) # TRUE, so it does currently have a long/lat CRS
crs(annual_2010) # v2024: GEOGCRS[\"WGS 84\",\n. The same as before `raster::calc`, time to move forward! 
```

# Rescale from 0 to 1

This pressure layer is rescaled so that all values lie between 0 and 1 using both a historical reference period and a biological reference point.

As a reminder, **supersaturated** conditions are when Ω \> 1, with an excess of \[ $CO_3^{2-}$ \]seawater, where a crystal of $CaCO_3$ will tend to grow.
**Undersaturated** conditions are when Ω \< 1, of which $CaCO_3$ will dissolve.

All cells with values less than one, indicating an undersaturated state, are set equal to the highest stressor level, 1.
For those that are supersaturated and greater than a value of 3, they will be set to a pressure score of 0.
For all other cells, rescaling the aragonite staturation state value to between 0 and 1 relies upon the change in saturation relative to the reference period (historical_avg).

Deviation from aragonite saturation state is determined for each year in the study period using this equation:

$$1 - (\frac{oapressure-1}{3-1})$$

And for all areas in which historical values are \> 1 or \<= 3:

$$\Delta \Omega_{year} = \frac{(\Omega_{hist} - \Omega_{current})}{(\Omega_{hist} - 1)}$$

Note that the current value is subtracted from the baseline(hist); this way, a reduction in $\Omega$ becomes a positive pressure value.
It is then normalized by the current mean state; so a decrease in $\Omega$ while the current state is high indicates less pressure than the same decrease when the current state is near 1.

Afterwards, $\Delta \Omega_{year}$ is then reclassified to account for increases in aragonite saturation state (when values are negative, pressure = 0) and aragonite sat state less than 1 (values are greater than 1, pressure = 1).

The `oaRescale` function rescales each of the annual rasters accordingly.

## Re-worked (v2024) Function for Rescaling

```{r}
annual_avg_rescaled_dir <- here::here(oa_dir, version_year, "int","annual_avg_rescaled") 
# dir.create(annual_avg_rescaled_dir) # create Mazu directory for rescaled rasters if not already created

# Read in historical raster data from earlier
historical_avg <- terra::rast(here::here(int_dir, "oa_1985_2000_historical_average.tif"))

# make a list of the already saved annual rasters
oa_annual_files <- list.files(path = here(oa_annual_mean_dir),
                              full.names = TRUE) # returns the full path, not just the file name

oaRescale_updated <- function(file){
  
  yr = as.numeric(substr(file, nchar(file)-7, nchar(file)-4)) # get year of file using the file's path/name
  
  oa_yr_avg = terra::rast(file) # rasterize annual mean aragonite raster for each given year 
  
  diff = (historical_avg - oa_yr_avg)/(historical_avg - 1) # deviation from aragonite saturation state for each year in the study period
  
  oa_pressure <- terra::ifel(oa_yr_avg <= 1, 1, oa_yr_avg) 
  # any saturation values that are less than 1 are undersaturated, so they would have the highest pressure possible (reclassify as 1)
  
  oa_pressure <- terra::ifel(oa_yr_avg > 3, 0, oa_pressure) 
  # any saturation values that are greater than 3 are way above supersaturation, so they would have the lowest pressure possible (reclassify as 0)
  
  oa_pressure <- terra::ifel(oa_pressure > 1 & oa_pressure <= 3, 1 - (oa_pressure - 1)/(3 - 1), oa_pressure)
  # for arag sat values between 1 and 3, the rescaled value would be 1 - (oa_pressure - 1)/(3 - 1)
  
  oa_pressure <- terra::ifel(historical_avg > 1 & historical_avg <= 3, diff, oa_pressure) 
  #  if the historic values also land between 1 and 3, we replace the linear interpolation values from `diff`
  
  oa_pressure <- terra::ifel(oa_pressure < 0, 0, oa_pressure) # any rescaled values less then 0 should be considered no pressure, because that means the current value was larger than the historical value, and therefore the aragonite sat increased in reference to the historical value

  oa_pressure <- terra::ifel(oa_pressure > 1, 1, oa_pressure) # any rescaled values greater than 1 should be considered 100% pressure, because that means the current value was smaller than the historical value, and therefore the aragonite sat decreased a lot in reference to the historical value
  
  writeRaster(oa_pressure, 
              filename = paste0(annual_avg_rescaled_dir, '/oa_rescaled_', yr, ".tif"),
              overwrite = TRUE)
  
}

# ---- apply the function to the list of annual mean files -----
lapply(oa_annual_files, oaRescale_updated)
```

Now, visualize the rescaled rasters and check that rescaling worked correctly.

```{r}
# plot the rescaled rasters to ensure it looks correct.  
ff <- list.files(path = here::here(annual_avg_rescaled_dir), pattern = '.tif$',
                 full.names = TRUE) 
oa_rescaled <- terra::rast(ff) # make into a multilayer spatraster so that all annual rasters can be seen
plot(oa_rescaled) # v2024: this looks great!
```

# Zonal Statistics and Results

## Reproject, resample, check extents, and mask

```{r, eval = FALSE}
# Examine the OHI eez raster
plot(zones) # examine the .tif and ensure it looks correct
crs(zones) # v2024: PROJCRS[\"Mollweide\",\n
res(zones) # 934.4789 934.4789, lower cell size = higher resolution
```

Now, let's reproject the rescaled rasters to `zones`, for OHI consistency

```{r}
# tic()
# oa_rescaled_moll <- terra::project(oa_rescaled, zones) # using terra::project, which inherently resamples using bilinear interpolation
# toc() 
annual_avg_rescaled_moll_dir <- here::here(oa_dir, "int","annual_avg_rescaled_moll") 
# dir.create(annual_avg_rescaled_moll_dir) # create Mazu directory for rescaled rasters if not already created

# make a list of the already saved and rescaled annual rasters
oa_rescaled_annual_files <- list.files(path = here(annual_avg_rescaled_dir),
                              full.names = TRUE) # returns the full path, not just the file name


# do not run in parallel because it is not compatible with terra::project() (package might be updated in the future to accommodate parallelization)
# reprojects each rescaled raster, sets the long/lat projection

reprojecting_func <- function(file){
  
  yr = as.numeric(substr(file, nchar(file)-7, nchar(file)-4)) # get year of file using the file's path/name
  
  # Read in eez raster with cells at 1km -- used for resampling and zonal statistics (held on an NCEAS server)
  zones <- terra::rast(here::here(dir_M, "git-annex", "globalprep","spatial","v2017","regions_eez_with_fao_ant.tif"))
  
  # # supress suxiliary files
  # setGDALconfig("GDAL_PAM_ENABLED", "FALSE")
  
  # read in annual raster
  r <- terra::rast(file[i])
  
  # reprojecting to the OHI zones file to ensure they have the same extent and proj
  r_moll <- terra::project(r, crs(zones), threads = TRUE)
  
  # resample to ensure they have the same resolution and write out the file to Mazu
  r_moll_resample <- terra::resample(x = r_moll, y = zones, method = "bilinear", filename = paste0(annual_avg_rescaled_moll_dir, '/oa_rescaled_moll_', yr, ".tif"), overwrite = TRUE)
  
}

tic()
lapply(oa_rescaled_annual_files, reprojecting_func)
toc() # v2024: 1047.173 sec elapsed
```

Now, visualize the reprojected rasters to ensure it looks correct.

```{r}
# plot the rescaled rasters to ensure it looks correct.  
reproj_annual_rasts <- list.files(path = here::here(annual_avg_rescaled_moll_dir), pattern = '.tif$',
                 full.names = TRUE) 
oa_reprojected <- terra::rast(reproj_annual_rasts) # make into a multilayer spatraster so that all annual rasters can be seen
plot(oa_reprojected) # v2024: this looks great!

ext(oa_reprojected) == ext(zones) # v2024: TRUE
res(oa_reprojected) == res(zones) # v2024: TRUE
crs(oa_reprojected) == crs(zones) # v2024: TRUE

# great! Now we can move on to zonal stats.
```

## Calculate Zonal Statistics

```{r}
# remember: `zones` from setup chunk
# zones is from `/home/shares/ohi/git-annex/globalprep/spatial/v2017`
# regions_eez_with_fao_ant.tif: This includes all the ocean regions (eez/fao/antarctica), but the raster cell values correspond to the rgn_ant_id in regions_2017_update.  This file is most often used to extract pressure values for each region.

# use zonal statistics to calculate the average score of all the cells within each region
tic()
regions_stats <- terra::zonal(oa_reprojected, zones, fun = "mean", na.rm = TRUE, progress = "text") %>%
  data.frame() %>%
  setNames(c("rgn_id", "scores_2010","scores_2011","scores_2012","scores_2013","scores_2014","scores_2015","scores_2016","scores_2017","scores_2018","scores_2019","scores_2020","scores_2021","scores_2022"))
toc() # v2024: 361.876 sec elapsed

# bring in all OHI regions and their names to compare to our zonal statistics and see what is missing
rgn_data <- read_sf(here(dir_M, "git-annex", "globalprep", "spatial", "v2017"), "regions_2017_update") %>%
  st_set_geometry(NULL) %>%
  dplyr::filter(rgn_type == "eez") %>%
  dplyr::select(rgn_id = rgn_ant_id, rgn_name)

# use `setdiff()` between the original zonal stats and the OHI regions to see if any regions are missing
setdiff(regions_stats$rgn_id, rgn_data$rgn_id) # v2024: good! only the high seas regions that are not within the 250 OHI regions: 260 261 262 263 264 266 267 269 270 272 273 274 275 276 277
setdiff(rgn_data$rgn_id, regions_stats$rgn_id) # v2024: integer(0)

# clean up the zonal statistics resulting dataframe
regions_stats_clean <- regions_stats %>%
  filter(rgn_id <= 250) %>% # to filter out zonal artifacts
  gather("year", "pressure_score", -1) %>% # convert into tidier format
  mutate(year = as.numeric(as.character(substring(year, 8, 11)))) %>% # replace year from "scores_yr" into "yr"
  mutate(pressure_score = format(pressure_score, scientific = FALSE)) # remove scientific notation formating

# write out acid.csv, which is in the same format as previous years.  All future .csv write-outs are in different formats than previous years
# write_csv(regions_stats_clean, "output/acid.csv")

# bringing the names in to allow for greater readability
regions_stats_join <- left_join(regions_stats_clean, rgn_data, by = "rgn_id") %>% 
  relocate(rgn_name, .after = rgn_id)
```

# Final Pressure Scores

Visualize the final scores!

```{r}
pressure_oa_plot <- regions_stats_join %>%
  plot_ly(x = ~year, y = ~pressure_score, color = ~rgn_name, 
          type = "scatter", mode = "lines") %>%
  layout(title = "Ocean Acidification Pressure Scores by Region",
         xaxis = list(title = "Year"),
         yaxis = list(title = "Pressure Score"))
pressure_oa_plot

# save plot
# htmlwidgets::saveWidget(pressure_oa_plot, file = "output/prs_oa_all_years_plot.html")
```

Save output files in `output_dir`, refer back to the setup chunk.

```{r plot_final}
# ----- save all data (commented out in case someone runs the whole script) ---------
# write_csv(regions_stats_join, "prs_oa_all_yrs.csv")

# group the data by year
yearly_data <- regions_stats_join %>%
  group_by(year) %>% # group by all years, in this case it should be 2010 - 2022
  group_split() # splits into a list of tibbles by the previous grouping.  It may be deprecated in the future... check documentation

# get a list of unique years
years <- regions_stats_join %>%
  pull(year) %>% # specifying the year column, similar to using the $
  unique() %>% # all unique values in that column
  sort() # by default will sort in ascending order, double check "years" to make sure.

# create a function to write CSV files in the output folder
write_csv_by_year <- function(data, year) {
  filename <- file.path("output", paste0("prs_oa_", year, ".csv"))
  write.csv(data, file = filename, row.names = FALSE)
}

# ----- save yearly data (commented out in case someone runs the whole script) ---------
## use map2 (which allows two vectors of the same length) to iterate over the split data and years to write individual csvs for each year
# map2(yearly_data, years, write_csv_by_year) 
```

# Citation information

**Product Citation:**

Please refer to our Technical FAQ for citing products: <http://marine.copernicus.eu/faq/cite-cmems-products-cmems-credit/?idpage=169>.

**DOI (product):**

<https://doi.org/10.48670/moi-00047>

**References:**

Chau, T. T. T., Gehlen, M., and Chevallier, F.: A seamless ensemble-based reconstruction of surface ocean pCO2 and air–sea CO2 fluxes over the global coastal and open oceans, Biogeosciences, 19, 1087–1109, <https://doi.org/10.5194/bg-19-1087-2022>, 2022.

Climate Change Indicators: Ocean Acidity (2024) EPA Climate Change Indicators. Available at: <https://www.epa.gov/climate-indicators/climate-change-indicators-ocean-acidity> (Accessed: 06 August 2024). 

Barker, S. & Ridgwell, A. (2012) Ocean Acidification. Nature Education Knowledge 3(10):21.  <https://www.nature.com/scitable/knowledge/library/ocean-acidification-25822734/> 
