---
title: 'OHI `r format(Sys.Date(), "%Y")`: Sea Surface Temperature Pressure Layer'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../workflow/templates/ohi_hdr.html'
  pdf_document:
    toc: true
---

# Summary

This script creates the Sea Surface Temperature (SST) layer for the global Ocean Health Index assessment.

------------------------------------------------------------------------

# Updates from previous assessment

Used updated 2022 data from NOAA

Huge overhaul of this notebook done by Carlo Broderick in 2023.

Please reference readme.txt

------------------------------------------------------------------------

# Data Source

**Data come from NOAA's NCEI:**

-   [CoRTAD version 6](https://www.ncei.noaa.gov/products/coral-reef-temperature-anomaly-database)

**Native Data Resolution**:

-   \~4km

**Description:**

-   There are two datasets for this prep, the SST and SSTA data. SST data are weekly SST values in a 4km grid for the entire ocean. SSTA are the same SST values as before with a fancy climatic mean subtracted from them. We use the SSTA values to identify if an anomaly has occured by calculating the SD of the SST data and counting the number of weeks the SSTA is above the SD of the SST data.

------------------------------------------------------------------------

# Methods

1.  Extreme events per year based calculated as number of times weekly SST exceeds SST Standard Deviation based on weekly values. This is done by calculating the SD for weekly temperature for each week from 1982 - 2011. This 30 year period was selected because it is the oldest 30 year period in the dataset, and 30 years is the length of time recommended by the World Meteorological Association to create a baseline climatological statistic. These 53 SD weekly rasters are then used to calculate how many times SSTA values exceed the calculated SD for that week of the year. SSTA data is SST data with the climatic mean subtracted, so any value above the SD we consider a positive anomaly.
2.  We then sum each 5 year period starting in 1985-1989 and going to the present data year. We use the first of these 5 year periods as the reference period. We chose to use 1985-1989 instead of 1982-1986 because of anomalies that occurred during the later time period.
3.  We then subtract the number of extreme events for each five year period from reference period (1985-1989). This yields a raster with cell values representing the dfference between the number of anomalies in the target period when compared to the reference period.
4.  Next, we rescale the surplus posative anomaly raster data to values between 0 and 1 by dividing each cell value by the 99.99th quantile among all cell values for all years of data. We also turn all negative values to 0.
5.  Finally we take this rescaled raster data and average the cell values within each OHI region for each OHI year to get each regions OHI SST pressure score!

# Updating prs_sst

Before starting this notebook, please read through the read me for guidance on how to begin and successfully complete this data prep.

## Setup

```{r setup, message=F,warning=F, eval = FALSE}

# Set options for code chunks
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'figs/', message = FALSE, warning = FALSE)

# Install ohi core if needed
#devtools::install_github('ohi-science/ohicore@dev')

# Load packages
library(raster)
library(RColorBrewer)
library(tidyverse)
library(rgdal)
library(doParallel)
library(foreach)
library(sf)
library(ncdf4)
library(httr)
library(lubridate)
library(animation)
library(here)
library(snow)
library(terra)
library(patchwork)

# Suppress progress bars for terra
terraOptions(progress=0)

# OHI spatial files, directories, etc
source(here("workflow/R/common.R"))

# Update scenario year, set up programmatic scenario year updating
scen_year_number <- 2023
scen_year <- as.character(scen_year_number)
prev_scen_year <- as.character(scen_year_number - 1)

# Standard OHI file paths
dir_data <- paste0(dir_M, "/git-annex/globalprep/_raw_data/CoRTAD_sst/d", scen_year)
dir_int  <- paste0(dir_M, "/git-annex/globalprep/prs_sst/v", scen_year, "/int")
dir_output  <- paste0(dir_M, "/git-annex/globalprep/prs_sst/v", scen_year, "/output")
dir_rasters <- paste0(dir_M, 
                      "/git-annex/globalprep/prs_sst/prs_sst_calculated_rasters/v2023_update")

# Load in OHI spatial data
ohi_rasters()
regions_shape()

# Specify years for calculations, the data set's years, often 1 year behind scenario year
yrs <- 1982:2022

# Rainbow color scheme and load in OHI regions
cols <- rev(colorRampPalette(brewer.pal(11, 'Spectral'))(255)) 
land <- regions %>% subset(rgn_type %in% c("land", "land-disputed", "land-noeez"))

```

------------------------------------------------------------------------

## Get new data if available

New data will have to be downloaded each year as the newest year of data is added to the data set.

```{r get new data, eval = FALSE}

# Define the base URL for the data
url <- "https://data.nodc.noaa.gov/cortad/Version6"

# Define the URL for the SSTA and WeeklySST data
weekly_sst <- sprintf("%s/cortadv6_WeeklySST.nc", url)
ssta <- sprintf("%s/cortadv6_SSTA.nc", url)

# Define the local file path for SSTA and WeeklySST data
weekly_sst_filename <- paste0(dir_M, "/git-annex/globalprep/_raw_data/CoRTAD_sst/d", 
                              scen_year, "/cortadv6_WeeklySST.nc")
ssta_filename <- paste0(dir_M, "/git-annex/globalprep/_raw_data/CoRTAD_sst/d", 
                        scen_year, "/cortadv6_SSTA.nc")

# Download the SSTA and WeeklySST and save it to the file path, (~100GB+, ~30GB+)
weekly_sst_res <- httr::GET(weekly_sst, write_disk(weekly_sst_filename))
ssta_res <- httr::GET(ssta, write_disk(ssta_filename))

# Close all open connections
closeAllConnections()

```

------------------------------------------------------------------------

## Generate annual positive anomalies

We consider positive SST anomalies any weekly temperature that is one standard deviation above the mean weekly temperature for that week of the year. We use 1 SD as the threshold to identify 'extreme events'. Since the sea surface temperature anomaly (SSTA) data downloaded from CoRTAD the temperature minus the mean, we calculate standard deviation and count cases where the anomaly data exceeds the standard deviation value. We use CoRTAD SSTA data instead of just calculating the mean ourselves because CoRTAD actually uses a fancy climactic mean that is not the same as the actual mean we would calculate from the SST data alone. However, we do calculate the SD from the SST data.

### Read and Format

This chunk reads in the data and creates a dataframe of the raster layer titles that is referenced later to call individual layers for calculation.

```{r read in and format data, eval = FALSE}

# Load netcdf for SSTA radiation data
ssta <- rast(list.files(dir_data, 
                              pattern = "SSTA.nc", 
                              full.names = TRUE), 
                   subds = "SSTA")


# Rename each layer, terra autonames so we take the names from layer dates
names(ssta) <- time(ssta)

# Assign names to an object
names_weekly <- names(ssta)

# Convert the names into a data frame
ssta_df <- data.frame(name = names_weekly) %>%
    # Convert the name to a date
    mutate(date = as.Date(name)) %>%
    # Extract the year, month, day, and week
    mutate(
        year = year(date),
        month = month(date),
        day = day(date),
        week = week(date))

```

------------------------------------------------------------------------

### Calculate new year anomalies

This chunk calculates how many anomalies occurred in the new year of data. It does this by reading in 53 raster files representing each week of the year and containing in each cell the SD of SST for that week of the year. If a specific year's weekly SST value is above that SD, then it is considered an anomaly, after all weeks are assessed, each weeks anomalies are then summed to create a single raster with the annual anomalies for that cell.

```{r calculate anomalies, eval = FALSE}

# Create array of years to calculate anomalies
target_years <- c(1985:max(yrs))

# Check to see which anomaly files already exist
anom_files <- list.files(paste0(dir_rasters, "/annual_positive_anomalies"), 
                         pattern = "annual_pos_anomalies", 
                         full.names = TRUE)

# Create array of anomaly file years
anom_file_years <- as.numeric(substr(anom_files, 137, 140))

# Find which anom years to calculate
yrs <- setdiff(target_years, anom_file_years)

# Loop runs each week in a year past the weekly SD rasters to identify and sum anomalies
# 53 Minutes
foreach(j = yrs, .packages = c("terra", "dplyr")) %dopar% {
  
  # Start timer
  start_time <- Sys.time()
  
  # Print a message indicating the year for which the anomaly is being calculated and the start time.
  print(paste("calculating anomaly for", j, "-- started at", start_time))
  
  # Filter 'ssta_df' for the current year 'j' and select the 'week' column. 
  # The result is stored in 'wks'.
  wks <- ssta_df %>% 
    filter(year == j) %>% 
    select(week)
  
  # initialize s
  s <- list()
  
  for(i in wks$week) {
    
    # Load a each week's SD raster
    sd_sst <- terra::rast(file.path(paste0(dir_rasters, "/weekly_sd_30yr_rasters", 
                                           sprintf("/sst_sd_week_%s.tif", i))))
    
    # Find the index of the current week in the 'names_ssta' vector for the current year
    w <- which(substr(names_weekly, 1, 4) == j)[i]
    
    # Get the 'w'-th raster layer from the 'ssta' stack.
    w_ssta <- ssta[[w]]
    
    # Find anomalies using custom function
    anomaly_raster <- terra::lapp(c(w_ssta, sd_sst),
                                  fun = function(x, y){ifelse(is.na(x) | is.na(y), 0,
                                                              ifelse(x > y, 1, 0))})
    
    # Fill s with anomaly rasters
    s[[i]] <- anomaly_raster
    
    # Print a message when it is at the 28th i loop
    if (i == 30) {
      print(paste("Halfway done with year", j))
    }
    
  } # End of inner loop

  # Stack the raster layers in 's' to create a multi-layered raster
  s_stack <- rast(s)

  # Use the 'sum_anom' function defined earlier to calculate the sum of the raster stack 's'.
  yr <- terra::app(s_stack, fun = sum)
  
  # Write the raster layer 'yr' to a TIFF file named "annual_pos_anomalies_sd_<year>.tif".
  writeRaster(yr, filename = file.path(dir_rasters, 
                                       "/annual_positive_anomalies", 
                                       sprintf("annual_pos_anomalies_sd_%s.tif", j)),
              overwrite=TRUE)
  
  # End timer and calculate elapsed time
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  
  # Print time message
  print(paste("Anom for year", j, "started at", start_time, "and took", elapsed_time, "minutes to complete"))
  
} # End of outer loop

```

------------------------------------------------------------------------

### Calculate new 5 year excess anomalies

This chunk creates a raster representing the number of excess anomalies that occurred in each cell during the last 5 years of data when compared to the reference anomaly period of 1985 - 1989.

```{r 5r excess anomalies, eval = FALSE}

# Create list of anom files to build 5 year anomaly raster
anom_files <- list.files(paste0(dir_rasters, "/annual_positive_anomalies"), 
                         pattern = "annual_pos_anomalies", 
                         full.names = TRUE)

# Create array of years to calculate 5yr anomalies for, 1986 - 89 is the first 5 yr period
target_5yr_years <- c(1990:prev_scen_year)

# Check to see which anomaly files already exist
anom_5yr_files <- list.files(paste0(dir_rasters, "/surplus_5yr_positive_anomalies"),
                             pattern = "sst_diff_ocean", 
                             full.names = TRUE)
                            

# Create array of anomaly file years
anom_5yr_files_years <- as.numeric(substr(anom_5yr_files, 138, 141))

# Find which anom years to calculate
yrs <- setdiff(target_5yr_years, anom_5yr_files_years)

# Save projection
anom_proj = "+proj=longlat +ellps=WGS84 +no_defs"

# Load land mask, assign mollCRS
land_mask <- land %>% st_geometry %>% st_transform(anom_proj)

# Convert the land_mask to a terra vector object.
land_mask_terra <- terra::vect(land_mask)

# This loop calculates the new 5 year anomaly
for(i in yrs){
  
  # Start timer
  start_time <- Sys.time()
  
  # Generate a sequence of 5 consecutive years starting from 'i'
  years <- (i - 4):i
  
  # Load reference anomaly period raster
  ref <- rast(file.path(paste0(dir_rasters, 
                               "/referenc_period_5yr_positive_anomalies/85_89_ref_anom.tif")))
  
  # Load OHI five year anomaly period raster stack
  five_year_rast <- rast(c(anom_files[substr(anom_files, 137, 140) %in% years]))

  # Subtract reference anoms from the target 5 year anoms
  r <- app(five_year_rast, fun=sum) - ref
  
  # Apply the land mask to the resulting raster and write it to a .tif file.
  r <- mask(r, land_mask_terra, inverse = TRUE) 

  # Write to file
  terra::writeRaster(r, file=sprintf("%s/sst_diff_ocean_%s-%s.tif", 
                                     paste0(dir_rasters,
                                            "/surplus_5yr_positive_anomalies"),
                               years[1], years[5]), overwrite=TRUE)
  
  # End timer and calculate elapsed time
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  
  # Print time message
  print(paste("Anom for years", years[1], "through", years[5], 
              "started at", start_time, "and took", elapsed_time, "to complete"))
  
} # End loop

```

------------------------------------------------------------------------

### Calculate new baseline for excess anomalies

This chunk takes all the calculated 5 year excess anomalies and extracts a baseline number that is later used to rescale the values in the excess anomalies rasters to be between 0 and 1. The baseline is the 99.99th percentile.

```{r reference point calculation, eval = FALSE}

# List excess anomaly files
diffs <- list.files(paste0(dir_rasters, "/surplus_5yr_positive_anomalies"), 
                    pattern = "diff", full.names = TRUE)

# Initiate empty vector
vals <- c()

# Forloop to concatenate all the values in all the anomaly files, 5 min
for(i in 1:length(diffs)){
  m = diffs[i] %>% 
    rast() %>% 
    values()
  vals = c(vals, m)
}

# Extract min, max, and 99.99th quantile from concatenated values

# Calculate minimum (-142 v2018; -159 v2021; -157 v2022; -166 v2023)
min_val   <- min(vals, na.rm = TRUE) 

# Calculate maximum (182 v2018; 228 v2021; 247 v2022)
max_val   <- max(vals, na.rm = TRUE) 

# Calculate 99.99th percentile (128 v2018; 148 v2021; 148 v2022, 159 v2023)
resc_num  <- quantile(vals, prob = 0.9999, na.rm = TRUE)

# Write the new reference point; only if changed since the last assessment
sup_info <- here(paste0("globalprep/supplementary_information/v", scen_year))

# Save new reference point to disk
rescale <- read.csv(file.path(sup_info, "reference_points_pressures.csv"))
 rescale$ref_point[rescale$pressure == "Sea Surface Temperature"] <- resc_num 
 write.csv(rescale, file.path(sup_info, "reference_points_pressures.csv"), row.names = FALSE)
 
```

------------------------------------------------------------------------

## Rescaling

This code chunk take the excess anomaly rasters and rescales them so their values are between 0 and 1, it does this by dividing all values by the 99.99th percentile calculated earlier or changing the value to 0 if the raster value is negative. Unlike the other chunks, this saves output to your scenario year folder. This is because each year will recalculate these values because the 99.99th percentile will change each year.

```{r rescale, eval = FALSE}

# Get list of files to rescale, excess anomaly files
diffs <- list.files(paste0(dir_rasters, "/surplus_5yr_positive_anomalies"), 
                    pattern = "diff.*tif", full.names = TRUE)

# Read in 99.99th percentile rescaling value
resc_num <- read.csv(file.path(sup_info, "reference_points_pressures.csv")) %>%
  filter(pressure == "Sea Surface Temperature") %>%
  .$ref_point
resc_num <- as.numeric(as.character(resc_num))

# Create output directory if it does not exist
if(!file.exists(dir_output)){dir.create(path = dir_output)}

# Loop over each file in the excess anomaly list, 3 minutes per year
for(i in 1:length(diffs)) {
  
  # Start timer
  start_time <- Sys.time()

  # Load the ith raster file from diffs
  r = terra::rast(diffs[i])

  # Extract the year from the filename
  y = substr(diffs[i], 133, 141)
  
  # Skip if file already exists
  if(file.exists(sprintf("%s/sst_%s_1985-1989.tif", dir_output, y))){
    print(paste("skipping", y))
    next
  } else {
    
    # Set the Coordinate Reference System (CRS) for the raster
    crs(r) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
    
    # Reproject the raster to Mollweide projection (mollCRS)
    out = terra::project(r, y = mollCRS, threads = TRUE)
    
    # Apply a function to rescale values between 0 and 1
    rescaled <- terra::app(out, fun = function(x){ifelse(x > 0,
                                                           ifelse(x > resc_num, 1, 
                                                                  x/resc_num), 0)})

    # Save the output to a new file, overwrite if the file already exists
    terra::resample(rescaled, ocean, method = "near",
                    filename = sprintf("%s/sst_%s_1985-1989.tif", dir_output, y),
                    overwrite = TRUE,
                    threads = TRUE)
  
    } # End if statement
  
  # End timer and calculate elapsed time
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  
  # Print time message
  print(paste("Nomalized anoms for years", y, "started at", start_time, 
              "and took", elapsed_time, "to complete"))
  
} # End loop

```

------------------------------------------------------------------------

## Zonal extraction of SST data

This code chunk applies an ice mask to each rescaled excess 5 yr anomalies raster file.

```{r load data for zonal extraction, eval = FALSE}

# Load zonal data, load and check relevant rasters
rast_loc <- file.path(dir_M, "git-annex/Global/NCEAS-Regions_v2014/data/sp_mol_raster_1km")
rgn_data <- read.csv(file.path(rast_loc, "regionData.csv")) 

# Load normalized anomaly raster list
sst_rasters <- list.files(dir_output, pattern = "sst_.*1985-1989.tif", full.names = TRUE)

# Apply ice mask
ice_mask <- terra::rast(raster("/home/shares/ohi/git-annex/Global/NCEAS-Pressures-Summaries_frazier2013/ice_mask_resampled"))

# Prepare for paralel processing, v2023 - took 10 minutes
registerDoParallel(2)

# This loop applies the ice mask to each rescaled excess 5yr anomaly
foreach(rast = sst_rasters) %do% {
  
  # Skip if file already exists
  #if(file.exists(sprintf("%s/sst_%s_rescaled_icemask.tif", dir_output, substr(rast, 64, 82)))){
  #  print(paste("skipping", substr(rast, 64, 82)))
  #  return()
  #} else {
    
    # Load rescaled excess 5yr anomaly 
    temp <- terra::rast(rast)
  
    # Apply ice mask
    temp <- temp * ice_mask
  
    # Write to disk
    terra::writeRaster(temp, 
          filename = file.path(sprintf("%s/sst_%s_rescaled_icemask.tif", dir_output, 
                                       substr(rast, 64, 82))),
          overwrite = TRUE)
  
    #} # End if statement
  
  # Print time stamp
  print(paste("completed", substr(rast, 64, 82), Sys.time()))
  
  # Clear memory
  gc()
  
} # End loop

```

### Visualize

This chunk creates and saves a .gif that allow you to visualize the most recent year of data in context with previous years.

```{r gif of rescaled ice masked sst, eval = FALSE}

# Get list of rescaled and masked sst data files
sst_res_mask <- list.files(dir_output, "sst.*_rescaled_icemask.tif", full.names = TRUE)

# Create gif visualizing the rescaled and masked sst
# This loop outputs warning and error text but successfully creates .gif
saveGIF({
  for(i in 1:length(sst_res_mask)){
    
    # Get name for image from file path date
    n = sprintf("SST Pressure %s", 
                substr(sst_res_mask[i], 64, 72))
    
    # Plot and save raster raster
    plot(raster(sst_res_mask[i]), 
         zlim = c(0, 1), # fix zlimits
         axes = FALSE, box = FALSE, 
         main = n)}}, 
  ani.width = 750,
  ani.height = 400,
  movie.name = sprintf("%s/sst.gif", dir_output)) 

```

### Zonal Extraction

Creates a data frame with the average cell value for each 5 year period associated with each OHI region.

```{r zonal extraction, eval = FALSE}

# Put together all rescaled icemasked rasters into one spat rast
sst_stack <- rast(list.files(dir_output, 
                              pattern = "sst_.*_rescaled_icemask.tif", 
                              full.names = TRUE))

# Rename becasue terra does a bad job at that
names(sst_stack) <- substr(list.files(dir_output, 
                              pattern = "sst_.*_rescaled_icemask.tif",
                              full.names = TRUE), 64, 72)

# Explore
plot(sst_stack[[nlyr(sst_stack)]])

# Extract data by OHI region, takes more than a few minutes
regions_stats <- terra::zonal(sst_stack, zones, fun = "mean", na.rm = TRUE,
                       progress = "text") %>% data.frame()

# This is a check that should yield antarctica, try plotting stuff, you'll see  
setdiff(regions_stats$regions_eez_with_fao_ant, rgn_data$rgn_id) 
setdiff(rgn_data$rgn_id, regions_stats$regions_eez_with_fao_ant)

# Wrangle and save
data <- merge(rgn_data, regions_stats, all.y = TRUE, by.x = "rgn_id", 
              by.y = "regions_eez_with_fao_ant") %>% 
  write.csv(file.path(dir_output, "rgn_sst_prs.csv"), row.names = FALSE)

# Read data back in
data <- read.csv(file.path(dir_output, "rgn_sst_prs.csv"), stringsAsFactors = FALSE)
```

## Write final pressure layer and gapfilling record

```{r sst pressure layer, eval = FALSE}
# This loops over every ohi year puts the pressure score in a df and saves as a csv
for(years in c(2012:max(yrs))){
  
  # OHI year
  scenario = sprintf("X%s.%s", years-4, years)
  
  # Add to eex datafrme
  eez = data %>% 
    filter(sp_type == "eez") %>% 
    select(rgn_id, contains(scenario)) %>% 
    rename(pressure_score = contains(scenario))
  
  # Save to output folder
  write.csv(eez, sprintf("output/sst_eez_%s.csv", years), row.names = FALSE)
}
```

```{r gapfilling record for most recent year of data, eval = FALSE}
# SST has no gapfilling...
sst <- read.csv("output/sst_eez_2022.csv")
sst <- mutate(sst, pressure_score_gf = 0)
write.csv(sst, "output/sst_eez_2022_gf.csv", row.names = FALSE)
```

# Save altogether

```{r, eval=FALSE}

# Initiate empty dataframe
sst_final <- data.frame()

# Loop over all years and great a csv will all data
for (year in 2012:max(yrs)){

    # Read in data
    prs <- read.csv(sprintf("output/sst_eez_%s.csv", year))
    
    # Select scores
    prs <- prs %>%
    mutate(year = year) %>%
    select(rgn_id, year, pressure_score)
    
    # Add scores to dataframe
    sst_final <- rbind(sst_final, prs)
  
} # End loop

# Save dataframe in output
write.csv(sst_final, "output/sst_updated.csv", row.names=FALSE)

```

------------------------------------------------------------------------

# Citation information

Selig, E.R., K.S. Casey, and J.F. Bruno (2010), New insights into global patterns of ocean temperature anomalies: implications for coral reef health and management, Global Ecology and Biogeography, DOI: 10.1111/j.1466-8238.2009.00522.x.
