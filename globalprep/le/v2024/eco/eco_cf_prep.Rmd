---
title: "OHI `r format(Sys.Date(), '%Y')` - CF ECO Revenue Preparation"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 1
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../../workflow/templates/ohi_hdr.html'
  pdf_document:
    toc: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

# CF Economies: USD Value of Marine (Commercial) Fishing per Country/Region per Year

## Overview

**FAO Capture Data (downloaded August 24, 2023)**

-   Data Source FAO Global Capture Production (in metric tonnes)

-   This version of the value database was downloaded from the
    Statistical Query Panel. Data from [FAO Global Capture
    Production](https://www.fao.org/fishery/en/collection/capture?lang=en)

-   Citation: FAO 2023. Global Capture Production. Fisheries and
    Aquaculture Division
    <https://www.fao.org/fishery/en/collection/capture?lang=en>

-   Source information: Navigate to the [online query
    portal](https://www.fao.org/fishery/statistics-query/en/capture/capture_quantity)
    for FAO Global Capture Production Quantity. Deselect all
    pre-selected years. Drag these fields into selected rows: ASFIS
    species name, FAO major fishing area name, ASFIS species ISSCAP
    group name En. ASFIS species Family scientific name, FAO major
    fishing areas, Inland/Marine areas Name en. Click on show data and
    confirm that data is present for 1950- two years prior to current
    year. Click download and select yes to include Null Values.

-   Date: September 15th, 2023

-   Time range: 1950-2021

-   Native data resolution: Country level

-   Format: csv

-   Description: Global Capture Production Quantity

**Ex-Vessel Price Data (downloaded August 24, 2023)**

-   Ex-vessel price data is in USD/metric tonne.

    -   ex-vessel-prices: ex-vessel prices for fishery caught species
        from 1976-2019

        -   exvessel_price_database_1976_2019.csv: ex-vessel price data
            gathered from [Melnychuk et al.
            2016](https://doi.org/10.1093/icesjms/fsw169) and updated to
            2019 using methods described in the public-facing [github
            repo](https://github.com/SFG-UCSB/price-db-sfg) associated
            with the Melnychuk et al. 2016 paper

    -   \*\*Citation for paper\*\*

    -   Melnychuk, M. C., Clavelle, T., Owashi, B., and
        Strauss, K. 2016. Reconstruction of global ex-vessel prices of
        fished species. - ICES Journal of Marine Science.
        <doi:10.1093/icesjms/fsw169>.

### Setup

```{r setup}

knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval=FALSE)

# load packages
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}
librarian::shelf(
  here,
  janitor,
  foreach,
  doParallel, # for using multiple cores, if needed
  tidyverse, 
  plotly,
  zoo, # for gapfilling
  pdftools, # for FAO yearbook
  ohicore,
  modelr, # needed for gapfilling exvessel price data
  ggplot2
)
#remotes::install_github("skgrange/threadr") # for na_extrapolate
library(threadr) # for gapfilling, if necessary

# ---- sources! ----
source(here("workflow/R/common.R")) # file creates objects to process data

source(here('workflow/R/fao_fxn.R')) # fxn for cleaning old version of FAO data 

source(here('workflow/R/fao_online_portal_clean.R')) # fxn for obtaining FAO data

# ---- set year and file path info ----
current_year <- 2024 # Update this in the future!!
version_year <- paste0("v",current_year)
data_dir_version_year <- paste0("d", current_year)
data_path <- here("globalprep", "le", version_year)

# ---- data directories ----

# Raw data directory (on Mazu)
raw_data_dir <- here::here(dir_M, "git-annex", "globalprep", "_raw_data")

# World Bank raw data directory
wb_dir <- here(raw_data_dir, "WorldBank", data_dir_version_year)

# ILO raw data directory
ilo_dir <- here(raw_data_dir, "ILOSTAT", data_dir_version_year)

# FAO raw data directory
fao_dir <- here(raw_data_dir, "FAO", data_dir_version_year)

# OECD raw data directory
oecd_dir <- here(raw_data_dir, "OECD", data_dir_version_year)

# FAO capture raw data directory
fao_capture_dir <- here(raw_data_dir, "FAO_capture", data_dir_version_year)

# Ex-vessel raw data directory
ex_vess_dir <- here(raw_data_dir, "ex-vessel-price-database-updated")

# output data dir for intermediate data products
int_dir <- here(data_path, "int")
```

### Read in Data

```{r read_data}
# FAO Capture raw data, downloaded July 10, 2024.  Data goes from 1950 - 2022. This was downloaded to exemplify what the raw FAO Capture data looks like, though it will not need to be used in this script.
fao_capture_raw <- read_csv(here(fao_capture_dir, "Global_capture_production_Quantity_1950-2021.csv"))

# Ex-Vessel raw data, from EmLab, who projected Melnycuk's data (see README, or overview)
exvessel_prices_raw <- read_csv(here(ex_vess_dir, "price-db-results", "exvessel_price_database_1976_2019.csv"))

# read in cf revenue data from v2023, that already used FAO Capture data and Ex-vessel data
## this will be used for v2024 as well, considering that the EmLab exvessel data is not projected further than 2019
cf_revenue <- read_csv(here("globalprep", "le", "v2023", "int", "cf_revenue.csv"))
```

#### v2023 Methodology to produce cf_revenue

*Taken from `livelihood_economies_dataprep.Rmd` in v2023*

FAO capture data is combined with ex-vessel price data to estimate total
revenue from fishing for each country.

Here we do some basic cleanup of the global capture production database.

```{r fao_capture_cleaning, eval = FALSE}
# this is code from the `livelihood_economies_dataprep.Rmd` in v2023, so that future reviewers can understand where the object cf_revenue came from (which is what will be used going forward), aka "cf_revenue.csv", the final output from v2023
fao_latest_data_year <- 2021 #update to the last year available in FAO data, likely the same for all FAO data included. 

# reading in FAO capture data and cleaning
fao_capture_clean <- fao_capture_raw %>% 
   dplyr::rename(country = "Country Name En",
                asfis_species = "ASFIS species Name En",
                area = "FAO major fishing area Name En",
                area_type = "Inland/Marine areas Name En",
                isscaap_group = "ISSCAAP group Name En",
                family_scientific = "Family Scientific name")

fao_capture_clean <- fao_capture_clean %>% 
  fao_online_portal_clean(initial_data_year = 1950, last_data_year = fao_latest_data_year) %>%  #function to clean fao data
  mutate(FAO_name = ifelse(!is.na(asfis_species), asfis_species, family_scientific)) %>% 
  filter(area_type == "Marine areas") %>% #we only want marine capture
  mutate(year = as.numeric(year)) %>% 
  filter(year >= 1976 & year <= 2019) %>% #filter to years in the price database
  filter(!is.na(value) & value > 0) %>% # we do not need the nas or 0, only want the ones with value for tonnes
  filter(`Unit Name` == "Tonnes - live weight") # we don't have price data for species measured in whole animals
```

Next we'll clean up and gapfill the exvessel price data. This has
exvessel price in usd/metric tonne.

```{r gapfill_exvessel, eval = FALSE}
# cleaning up exvessel price data
exvessel_prices_clean <- exvessel_prices_raw %>% 
  clean_names() %>% 
  mutate(year = as.numeric(year)) %>% 
  group_by(asfis_species, scientific_name, pooled_commodity, isscaap_group, isscaap_division, year) %>% 
  summarize(exvessel = mean(exvessel, na.rm = TRUE)) %>%   #there are some duplicate prices for a species-average when this is the case
  filter(!(is.na(year) & is.na(exvessel))) %>% #we can't use rows that are na for price and year
  dplyr::group_by(asfis_species) %>%
  #counts the numbers of non-missing values for each country (logical TRUEs regarded as one)
  dplyr::mutate(value_num = sum(!is.na(exvessel))) %>% 
  filter(value_num > 0) #the minimum we have is 8 years

#look at the data to see how price and year are related overall
model <- lm(exvessel~year, data = exvessel_prices_clean)
summary(model)
#very significant though does not explain a high % of the variation 

#look at linear model by individual species

#define all of the unique species 
species <- unique(exvessel_prices_clean$asfis_species)

#take a look at it at the individual species level
model_list <- list()

#loop through all of the unique species and run the model, store the results in a list
for (i in seq_along(species)) {
  
  species_current <- species[i]
   
  new <- exvessel_prices_clean %>% 
    filter(asfis_species == species_current)
  
  model <- lm(exvessel~year, data = new)
  summary <-summary(model)
  coefficients <- summary$coefficients %>% 
    as.data.frame()
  model_list[[i]] <-coefficients
}

model_test <- bind_rows(model_list)

# Filter rows based on row names containing "year", only want significance of year
filtered_data <- model_test[grep("year", rownames(model_test)), ] %>% 
  clean_names() 
not_sig <- filtered_data %>% filter(pr_t > 0.05) 

(nrow(filtered_data) - nrow(not_sig)) / nrow(filtered_data)
#roughly 85% of these have a significant relationship, appears to be a reasonable gapfilling method

#use lm to predict the value for years which are missing data 
#based on code from ao_need data prep
price_model <- function(df) {lm(exvessel~year, data = df)}

# use the lm() to gapfill 
exvessel_prices_gapfilled <- exvessel_prices_clean %>% 
  dplyr::group_by(asfis_species) %>% 
  tidyr::nest() %>% 
  dplyr::mutate(
    ### Apply the model to all country groupings
    model = purrr::map(data, price_model),
    # Use the trained model to get predicted values
    predictions = purrr::map2(data, model, add_predictions)) %>% 
  tidyr::unnest(cols = c(predictions)) %>% 
  dplyr::select(-data, -model, prediction = pred) %>%
  dplyr::mutate(
    gapfilled = dplyr::case_when(is.na(exvessel) | value_num == 1 ~ 1, T ~ 0),
    exvessel = dplyr::case_when(is.na(exvessel) ~ prediction, T ~ exvessel),
    method = dplyr::case_when(
      value_num == 1 ~ "gapfilled using one year of data",
      gapfilled == 1 & value_num > 1 ~ paste0("lm based on N years data: ", value_num),
      T ~ as.character(NA))) %>% 
  dplyr::ungroup() %>% 
  filter(exvessel > 0) #remove price less than 0. This doesn't make sense and never occurs after 1998 anyway so would not be used to calculate any scores. 
```

Next, the exvessel data that has now been gapfilled will be joined with
the fao capture data.

```{r combine_exvessel, eval = FALSE}
#combine the price data with the capture data 
fao_capture_price <- fao_capture_clean %>% 
  left_join(exvessel_prices_gapfilled, by = c("asfis_species", "year", "isscaap_group")) %>% 
  select(-prediction)

#7% of observations still have NAS
summary(fao_capture_price)

#if still NA fill with average price for isscaap species group for that year

#find the average price for each group/year in the exvessel data
group_average_price_year <- exvessel_prices_gapfilled %>% 
  group_by(isscaap_group,year) %>% 
  summarize(mean_group_price = mean(exvessel))

# finish the join
fao_capture_price_final <- 
  fao_capture_price %>% 
  left_join(group_average_price_year, by = c("isscaap_group", "year")) %>% 
  mutate(final_price = ifelse(!is.na(exvessel), exvessel, mean_group_price)) %>%   
  mutate(gapfilled = ifelse((is.na(exvessel) & !is.na(final_price)), 1, gapfilled)) %>% 
  mutate(method = ifelse((is.na(exvessel) & !is.na(final_price)), "filled based on iscaap group average", method)) 

summary(fao_capture_price_final)
#there are still some NAS here, 2% 
nas <- fao_capture_price_final %>% 
  filter(is.na(final_price))
unique(nas$isscaap_group)
#after group discussion we decided to leave these as NA, due to lack of available data
#some of these species including turtles and corals are also considered unsustainable


fao_capture_price_final <- fao_capture_price_final %>% 
  select(country, year, value, asfis_species, final_price, gapfilled, method) %>% 
  mutate(revenue = final_price * value) # multiply price per tonne by number of tonnes

summary(fao_capture_price_final)

#sum prices by country for each year
fishing_revenue <- fao_capture_price_final %>% 
  group_by(country, year) %>% 
  summarize(value = sum(revenue, na.rm = TRUE)) %>% 
  mutate(unit = "USD (1)",
         sector = "cf",
         data_source = "FAO capture production and ex-vessel prices")

# save intermediate data!
# write_csv(fishing_revenue, here(data_path, "int/cf_revenue.csv"))
```

### Run `ohicore::name_2_rgn`

```{r name_2_rgn}
# check how many countries are available from the v2023 .csv before running names_2_rgn
length(unique(cf_revenue$country)) # v2024: 203

# run names_2_rgn, which identifies country names from the cf_revenue and determines which OHI region it belongs to
cf_revenue_rgn <- ohicore::name_2_rgn(df_in = cf_revenue, 
                             fld_name='country',
                             flds_unique = c("year"))

# to determine the correct OHI regions and compare the function's actions:
region_names <- read_csv("https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/eez/spatial/regions_list.csv") %>% select(-Notes)
```

When names_2_rgn has been run, there are some countries from cf_revenue
that are removed due to not having any match in the lookup tables. This
includes: bonaire/s.eustatius/saba, british indian ocean ter, channel
islands, french southern terr, isle of man, netherlands antilles, other
nei, saint barthélemy, saint helena/asc./trist., saint-martin (french),
yugoslavia sfr.

Some could be considered OHI regions, but they are aggregated within
another name. This includes: Bonaire/S.Eustatius/Saba, which can be
split into Bonaire, Sint Eustatius, and Saba; and the Channel Islands,
which should be split into Jersey and Guernsey.

Others can be dropped because they do not belong to an OHI region. This
includes yugoslavia sfr, saint barthélemy, other nei, isle of man
(self-governed), and french southern terr.

```{r fix_rgns}
# fix the country names so they can be recognized by names_2_rgn
cf_revenue_fix <- cf_revenue %>%
  mutate(country = case_when(
    country=="British Indian Ocean Ter" ~ "British Indian Ocean Territory",
    country=="Saint Helena/Asc./Trist." ~ "Saint Helena", 
    country=="Saint-Martin (French)" ~ "Northern Saint-Martin",
    country=="Netherlands Antilles" ~ "Curacao", #Sint Maarten has already been covered in a separate country name, so this must be referring to Curacao
    country=="Puerto Rico" ~ "Puerto Rico and Virgin Islands of the United States", #to ensure the fxn aggregated correctly
    country=="US Virgin Islands"~"Puerto Rico and Virgin Islands of the United States", #to ensure the fxn aggregated correctly
    country=="China, Hong Kong SAR" ~ "China", #to ensure the fxn aggregated correctly
    country=="China, Macao SAR" ~ "China", #to ensure the fxn aggregated correctly
    country=="United Republic of Tanzania, Zanzibar" ~ "Tanzania", #to ensure the fxn aggregated correctly
    country=="Tanzania, United Rep. of" ~ "Tanzania", #to ensure the fxn aggregated correctly
    TRUE ~ country # Everything else, leave it be
  ))
```

Re-run names_2_rgn after fixing names.

```{r fix_names}
# re-run names_2_rgn, check which countries are still dropped
cf_revenue_rgn_fix <- ohicore::name_2_rgn(df_in = cf_revenue_fix, 
                             fld_name='country',
                             flds_unique = c("year"))

# we are left with regions that are not associated with OHI, and also some that need to be disaggregated so that name_2_rgn can recognize them. Let's do that:
```

## Disaggregation

**Regions to disaggregate:**

Channel Islands ➞ Guernsey (228) and Jersey (227)

Bonaire/S.Eustatius/Saba ➞ Bonaire (245), Sint Eustatius (249), and Saba
(248)

To do this, look at the rasterized fishing effort calculated in 2017
within the FIS subgoal, to determine the relative proportion of
mean_catch within each region's eez. Afterwards, distribute revenue
relative to that proportion.

```{r proportions}
# load in data on the mean catch for each region, used as a proxy to determine which region has a greater commercial fishing economy
mean_catch_2017 <- read_csv("https://raw.githubusercontent.com/OHI-Science/ohiprep_v2024/gh-pages/globalprep/fis/v2020/int/mean_catch.csv")

# function to calculate average catch for certain regions and certain years
calculate_avg_catch <- function(data, region_ids, years) {
  data %>% 
    select(-stock_id_taxonkey) %>% 
    filter(rgn_id %in% region_ids, year %in% years) %>% # for the specified region id and year:
    group_by(rgn_id, year) %>%
    summarize(catch = sum(mean_catch, na.rm = TRUE)) %>% # summarize the mean catch for all species per rgn_id and year within the sequence of years
    ungroup() %>%
    group_by(rgn_id) %>% #for all of the years per region, find the average catch using the mean catch of each year
    summarize(avg_catch = mean(catch)) %>%
    select(rgn_id, avg_catch) %>% 
    dplyr::distinct() %>%  # ensures that there's only one row per region, removing any potential duplicates
    pivot_wider(names_from = "rgn_id", values_from = "avg_catch") # to call on specific observations more easily
}

# define parameters to input into the functions
years_to_analyze <- 2014:2017
channel_islands_ids <- c("228", "227")
bes_islands_ids <- c("245", "249", "248")

# calculate proportion of average catch for Channel Island regions for the 3 most recent years
mean_catch_ci_agg <- calculate_avg_catch(mean_catch_2017, channel_islands_ids, years_to_analyze) %>%
  rename(jersey = `227`, guernsey = `228`) %>% 
  pivot_longer(cols = c(jersey, guernsey),
             names_to = "rgn_id",
             values_to = "avg_catch") %>% 
  mutate(prop = avg_catch/sum(avg_catch, na.rm = TRUE))

# ccalculate proportion of average catch for Bonaire/S.Eustatius/Saba
mean_catch_bes_agg <- calculate_avg_catch(mean_catch_2017, bes_islands_ids, years_to_analyze) %>%
  rename(bonaire = `245`, sint_eustatius = `249`, saba = `248`) %>% 
  pivot_longer(cols = c(bonaire, sint_eustatius, saba),
             names_to = "rgn_id",
             values_to = "avg_catch") %>% 
  mutate(prop = avg_catch/sum(avg_catch, na.rm = TRUE))

# calculate proportions for each disaggregated region
## Channel Islands
prop_j <- mean_catch_ci_agg$prop[1]
prop_g <- mean_catch_ci_agg$prop[2]

## Bonaire/S. Eustatius/Saba
prop_b <- mean_catch_bes_agg$prop[1]
prop_e <- mean_catch_bes_agg$prop[2]
prop_s <- mean_catch_bes_agg$prop[3]
```

Now, use the proportions to disaggregate.

```{r disaggregation}
# use proportions to disaggregate Channel Islands 
split_ci <- cf_revenue %>%
  filter(country == "Channel Islands") %>%
  mutate(split_col = "Guernsey;Jersey") %>%
  separate_rows(split_col, sep = ";") %>%
  group_by(country, year, value, unit) %>%
  mutate(new_value = case_when(
    split_col == "Guernsey" ~ value * prop_g,
    split_col == "Jersey" ~ value * prop_j,
    TRUE ~ NA_real_
  )) %>% # multiply each value by its proportion of avg catch in the area to split the value appropriately
  ungroup() %>%
  mutate(country = split_col) %>%
  select(-split_col)

# split for Bonaire/S.Eustatius/Saba
split_bes <- cf_revenue %>%
  filter(country == "Bonaire/S.Eustatius/Saba") %>%
  mutate(split_col = "Bonaire;Sint Eustatius;Saba") %>%
  separate_rows(split_col, sep = ";") %>%
  group_by(country, year, value, unit) %>%
  mutate(new_value = case_when(
    split_col == "Bonaire" ~ value * prop_b,
    split_col == "Sint Eustatius" ~ value * prop_e,
    split_col == "Saba" ~ value * prop_s,
    TRUE ~ NA_real_
  )) %>% # multiply each value by its proportion of avg catch in the area to split the value appropriately
  ungroup() %>%
  mutate(country = split_col) %>%
  select(-split_col)

# merge with cf_revenue
cf_revenue_disaggregated <- cf_revenue_fix %>%
  bind_rows(split_ci, split_bes) %>% # bind with the disaggregated dataframes
  mutate(value = case_when(
    country %in% c("Guernsey", "Jersey") ~ new_value,
    country %in% c("Bonaire", "Sint Eustatius", "Saba") ~ new_value,
    TRUE ~ value
  )) %>% # if disaggregated, use the new value
  select(-new_value) # not needed anymore
```

Use a final names_2_rgn after disaggregation.

```{r final_names}
# using disaggregated revenue data within the name_2_rgn fxn: looks good!!
# seeing the aggregated versions of regions that we already disaggregated drop out. Great!
cf_revenue_rgn_final <- name_2_rgn(df_in = cf_revenue_disaggregated, 
                             fld_name='country',
                             flds_unique = c("year"))

#========== Indexing to check fxn ===============

# to see how many duplicates there are after names_2_rgn
rev_duplicates <- cf_revenue_rgn_final[duplicated(cf_revenue_rgn_final[, c("country", "year")]),]

# look at the unique duplicates for country and rgn_name to see if the function worked well
# setdiff to determine which observations are not the same in the country and rgn_id duplicates
rev_diff <- setdiff(unique(rev_duplicates$rgn_name), unique(rev_duplicates$country))
rev_diff # none! Move on

#=========== Aggregate by rgn_id and year ==============
cf_revenue_rgn_agg <- cf_revenue_rgn_final %>% 
  select(rgn_id, rgn_name, year, value, unit, sector, data_source) %>% 
  group_by(rgn_id, rgn_name, year) %>% # because china needs to be aggregated and has duplicate years
  dplyr::summarize(value = sum(value, na.rm=TRUE)) %>% # sum the value for each region and year
  ungroup() %>% 
  dplyr::mutate(year = as.numeric(year)) # to ensure it is numeric
```

## CF Revenue Plot

```{r rev_plot}
cf_rev_plot <- plotly::plot_ly(cf_revenue_rgn_agg, x = ~year, y = ~value, color = ~rgn_name, type = "scatter", mode = "lines") %>% 
  layout(title = "All Regions: Yearly Revenue for the Marine Fishing Sector", 
         xaxis = list(title = "Year"),
         yaxis = list(title = "Revenue of all catch (USD)"))
cf_rev_plot

# looks great! China has a very large revenue, which makes sense.  Indonesia and Peru are the have the next highest revenue in 2019.  Interesting to see a decrease in the early 2000s for most regions.
```

## Join with a "year" tibble df --in the future, replace this with `reframe()`!!

```{r year_join}
# make all geographic areas have the same date range (and fill missing values with NAs) to avoid problems downstream
years_df <- tibble(rgn_name = cf_revenue_rgn_agg$rgn_name) %>% 
  mutate(rgn_id = cf_revenue_rgn_agg$rgn_id) %>% 
  group_by(rgn_id, rgn_name) %>% 
  summarize(year = seq(min(cf_revenue_rgn_agg$year),
                       max(cf_revenue_rgn_agg$year)))

# ======== join with cf revenue aggregated data ========
cf_revenue_yrs <- left_join(years_df, cf_revenue_rgn_agg,
                          by = c("rgn_id", "rgn_name", "year"))

```

## Heatmap before Gapfilling

Typically, this would be used to visualize the data before and after
gapfilling, to get a better picture of what areas needed to be filled
and whether it was done correctly. However, here we are taking from
v2023's .csv that had already been gapfilled, so we are only going to
visualize how well the gapfilling worked in this case.

```{r interactive_heatmap}
# make a df where the "status" of the data (present or not) is defined.  Here, it would be 
heatmap_df <- cf_revenue_yrs %>% 
  mutate(status = !is.na(value))  #so that if there is data, the heatmap will be filled in

# make an interactive heatmap using ggplotly
heatmap_plot <- ggplot(data = heatmap_df) +
  geom_tile(aes(x = year,
                y = rgn_name,
                fill = status, 
                color = "black")) +
  scale_fill_manual(values = c("cornflowerblue", "gainsboro")) +
  labs(x = "Year",
       y = "Region Names",
       fill = "Data Status",
       color = " ") +
  theme_bw()
interactive_heatmap <- plotly::ggplotly(heatmap_plot)
# interactive_heatmap
```

It looks as though Norfolk Island may need to be cut, and a couple of
other regions need to be gap filled now that we joined with a data frame
of all possible years. Let's use a rule that we defined as a team, so
that cutting regions is consistent across sectors.

## Cleaning for GF

Rule in order to be preserved:

-   must have at least one data point in the last five years &

-   must have more than three data points over time

Otherwise, the region needs to be removed.

All regions preserved can be gapfilled if needed.

```{r cut_rgns}
# ======= Determining regions that need to be cut based on the above rule ========
start_year <- 2008
stop_year <- 2019
date_range <- start_year:stop_year
last_five_years <- (stop_year - 4):stop_year

# ---- first sub-rule: must have at least one data pt in the last five years ----
one_data_pt <- heatmap_df %>%
  group_by(rgn_name) %>%
  summarize(
    has_recent_data = any(year %in% last_five_years & status == "TRUE"),
    total_data_points = sum(status == "TRUE"),
  ) %>%
  ungroup() %>% 
  filter(has_recent_data == "FALSE") %>%
  pull(rgn_name)

# ---- second sub-rule: must have more than three data points over time ----
three_pts <- heatmap_df %>%
  group_by(rgn_name) %>%
  summarize(
    has_recent_data = any(year %in% last_five_years & status == "TRUE"),
    total_data_points = sum(status == "TRUE"),
  ) %>%
  ungroup() %>% 
  filter(total_data_points < 3) %>%
  pull(rgn_name)

# see which and how many regions must be filtered out
one_data_pt # v2024: Norfolk Island
length(one_data_pt) # v2024: 1

three_pts # v2024: NA
length(three_pts) # v2024: 0

# determine regions to keep
all_regions <- unique(heatmap_df$rgn_name) # a vector of all regions
regions_to_keep <- setdiff(all_regions, one_data_pt) # remove the regions that do not follow the rule

# filter the original dataframe to keep only the valid regions
filtered_heatmap_df <- heatmap_df %>%
  filter(rgn_name %in% regions_to_keep)
```

## Gapfilling

Here we'll use `na.approx()` from {zoo} to interpolate missing values
and extrapolate missing extremes (NA values that do not fall between
non-NA values) by copying the nearest extreme value.

Since we already filtered out any region that does not adhere to the
rule above, any regions left that have NAs can be gapfilled.

```{r}
# estimate value for regions with missing data
value_filled <- filtered_heatmap_df %>% 
  group_by(rgn_name) %>% 
  # interpolate (fill missing values between 2 values)
  mutate(appx_value = zoo::na.approx(value, # using values in this column
                                     na.rm = FALSE, # don't replace (internal) NAs in new column that can't be approximated
                                     #  extrapolate using rule = 2 from approx(),
                                     # which uses closest data extreme to
                                     #  extrapolate for leading and trailing NAs
                                     rule = 2
  )) %>% 
  ungroup()

# for the British Indian Ocean Territory in 2019, it copied the value from 2018.
# for Saba, anything before 2011 was filled with the 2011 value.  The exact same thing happened for Bonaire and Sint Eustatius, which makes sense because those three were disaggregated and therefore it makes sense that they would have the same data availability, as they came from the same source.
```

### Heatmap to check gapfilling

```{r heatmap_check}
heatmap_gf_df <- value_filled %>% 
  select(-c(value, status)) %>% 
  mutate(status = !is.na(appx_value))

heatmap_gf <- ggplot(data = heatmap_gf_df) +
  geom_tile(aes(x = year,
                y = rgn_name,
                fill = status, 
                color = "black")) +
  scale_fill_manual(values = c("cornflowerblue", "gainsboro")) +
  labs(x = "Year",
       y = "Region Names",
       fill = "Data Status",
       color = " ") +
  theme_bw()
interactive_heatmap_gf <- plotly::ggplotly(heatmap_gf)
# interactive_heatmap_gf

# YAY! it worked the way we wanted it to.  The heatmap is a solid color; there are only TRUEs, which indicates that there is a value present for every observation.
```

## Implementing Sector Multipliers

Refer to the OHI Methods
<https://ohi-science.org/ohi-methods/goals/goal-models-data.html#livelihoods-and-economies>
or [Ben Halpern's Supplementary
Information](https://static-content.springer.com/esm/art%3A10.1038%2Fnature11397/MediaObjects/41586_2012_BFnature11397_MOESM79_ESM.pdf)
**(page 29)** for the multipliers to apply towards revenue values by
sector. For commercial fishing revenue, the multiplier is 1.568 for
developed and developing countries.

```{r}
# As described in table 6.10: "Sector-specific multipliers are used to calculate total jobs and total revenue created by sector-based employment in developing and developed nations."

# -------- implement multiplier ------------
cf_multiplier <- heatmap_gf_df %>% 
  mutate(mult_value = appx_value * 1.568) %>% #multiply all values with the multiplier to account for ripple effects commercial fishing may have on revenue
  select(-appx_value)
```

## Saving intermediate data

```{r}
# columns should be: rgn_id, rgn_name, year, usd (previously value), unit, sector, usd_yr (in this case = year)

cf_revenue_int <- cf_multiplier %>% 
  mutate(usd = mult_value,
         unit = "USD (1)",
         sector = "cf",
         usd_yr = year) %>% 
  select(-c(mult_value, status))

# write_csv(cf_revenue_int, here(int_dir, "eco_cf_usd_pre.csv"))
```
